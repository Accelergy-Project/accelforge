{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e2a43f3",
      "metadata": {},
      "source": [
        "### Model of \"A 32.2 TOPS/W SRAM Compute-in-Memory Macro Employing a Linear 8-bit C-2C Ladder for Charge Domain Computation in 22nm for Edge Inference\", VLSI 2022\n",
        "\n",
        "Paper by Hechen Wang, Renzhi Liu, Richard Dorrance, Deepak Dasalukunte, Xiaosen\n",
        "Liu, Dan Lake, Brent Carlton, and May Wu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e465e48f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from _scripts import (\n",
        "    display_important_variables,\n",
        "    get_spec,\n",
        "    bar_comparison,\n",
        "    bar_stacked,\n",
        "    bar,\n",
        ")\n",
        "display_important_variables('wang_vlsi_2022')\n",
        "get_spec('wang_vlsi_2022').arch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a652cef4",
      "metadata": {},
      "source": [
        "#### Description of The Macro\n",
        "\n",
        "The macro uses a 512-row by 128-column SRAM array. Every eight adjacent columns\n",
        "store 8b weight slices and together compute one 8b MAC operation. An 8b voltage\n",
        "DAC provides 8b inputs in a single cycle. An analog-digital C-2C multiplier\n",
        "computes a MAC operation between an 8b analog input and an 8b digital weight.\n",
        "These multipliers allow the macro to compute 8b MACs in a single cycle and read\n",
        "the results with one ADC convert. Furthermore, they allow the macro to avoid\n",
        "connecting memory cells to analog circuits.\n",
        "\n",
        "Every eight rows in the array share a C-2C multiplier. These rows are activated\n",
        "in separate cycles, so it requires eight cycles to activate all rows in the\n",
        "array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e654dbb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import accelforge as af\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b448caf",
      "metadata": {},
      "source": [
        "#### Area Breakdown\n",
        "\n",
        "This test replicates the results of Fig. 22(b) of the paper.\n",
        "\n",
        "We show the area breakdown of the macro. The area is broken down into the\n",
        "following components:\n",
        "\n",
        "- ADC: Area consumed by the ADC\n",
        "- DAC: Area consumed by the DAC\n",
        "- MAC: Area consumed by the MAC, including the row drivers, select wordline\n",
        "    drivers, CiM unit, and C-2C multiplier.\n",
        "- Misc: Area consumed by the weight drivers and control circuitry.\n",
        "\n",
        "Modeled miscellaneous area is lower than reference because we do not model\n",
        "the control circuitry in the weight drivers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7ce1a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluated = get_spec('wang_vlsi_2022').calculate_component_area_energy_latency_leak()\n",
        "\n",
        "area = evaluated.arch.per_component_total_area\n",
        "\n",
        "TOTAL_AREA = 0.124e-6\n",
        "\n",
        "expected_area = {\n",
        "    \"ADC\": 0.13 * TOTAL_AREA,\n",
        "    \"DAC\": 0.3 * TOTAL_AREA,\n",
        "    \"MAC\": 0.46 * TOTAL_AREA,\n",
        "    \"Misc\": 0.11 * TOTAL_AREA,\n",
        "}\n",
        "modeled = {}\n",
        "modeled[\"ADC\"] = area[\"ADC\"] + area[\"ColumnDrivers\"] + area[\"ColumnBandwidthLimiter\"]\n",
        "modeled[\"DAC\"] = area[\"DAC\"]\n",
        "modeled[\"MAC\"] = area[\"CimUnit\"] + area[\"C2CMultiplier\"] + area[\"C2CMultiplierPortB\"] + area[\"RowDrivers\"] + area[\"SelectWordlineDrivers\"]\n",
        "modeled[\"Misc\"] = sum(area.values()) - sum(modeled.values()) + area[\"WeightDrivers\"]\n",
        "total_area = sum(modeled.values())\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "bar_comparison(\n",
        "    {\n",
        "        \"Modeled\": modeled,\n",
        "        \"Expected\": expected_area,\n",
        "    },\n",
        "    \"Component\",\n",
        "    \"Area (mm^2)\",\n",
        "    \"Area Breakdown: Modeled vs Expected\",\n",
        "    ax,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a29514",
      "metadata": {},
      "source": [
        "#### Energy Breakdown\n",
        "\n",
        "This test replicates the results of Fig. 22(a) of the paper. We show the energy\n",
        "breakdown of the macro. The energy is broken down into the following components:\n",
        "\n",
        "- ADC: Energy consumed by the ADC\n",
        "- DAC: Energy consumed by the DAC\n",
        "- MAC: Energy consumed by the MAC, including the row drivers, select wordline drivers,\n",
        "  CiM unit, and C-2C multiplier.\n",
        "- Misc: The weight drivers are miscellaneous components in our model, but they consume\n",
        "  no energy in this weight-stationary test. Misc also includes control circuitry in the\n",
        "  reference.\n",
        "  \n",
        "Modeled miscellaneous energy is lower than reference because we do not model the control\n",
        "circuitry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c5a41b",
      "metadata": {},
      "outputs": [],
      "source": [
        "spec = get_spec('wang_vlsi_2022', add_dummy_main_memory=True)\n",
        "spec.mapper.ffm.metrics = af.mapper.FFM.Metrics.ENERGY\n",
        "results = spec.map_workload_to_arch()\n",
        "energy = results.energy(per_component=True)\n",
        "\n",
        "TOPS_PER_WATT = 16.37572276\n",
        "PJ_PER_MVM = 2 / TOPS_PER_WATT * 16 * 64# * 8\n",
        "\n",
        "expected_energy = {\n",
        "    \"ADC\": PJ_PER_MVM * 0.34 * 1e-12,\n",
        "    \"DAC\": PJ_PER_MVM * 0.22 * 1e-12,\n",
        "    \"MAC\": PJ_PER_MVM * 0.4 * 1e-12,\n",
        "    \"Misc\": PJ_PER_MVM * 0.04 * 1e-12,\n",
        "}\n",
        "\n",
        "modeled = {}\n",
        "modeled[\"ADC\"] = energy[\"ADC\"] + energy[\"ColumnDrivers\"] + energy[\"ColumnBandwidthLimiter\"]\n",
        "modeled[\"DAC\"] = energy[\"DAC\"]\n",
        "modeled[\"MAC\"] = energy[\"CimUnit\"] + energy[\"C2CMultiplier\"] + energy[\"C2CMultiplierPortB\"] + energy[\"RowDrivers\"] + energy[\"SelectWordlineDrivers\"]\n",
        "modeled[\"Misc\"] = sum(energy.values()) - sum(modeled.values()) + energy[\"WeightDrivers\"]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "bar_comparison(\n",
        "    {\n",
        "        \"Modeled\": modeled,\n",
        "        \"Expected\": expected_energy,\n",
        "    },\n",
        "    \"Component\",\n",
        "    \"Energy (J)\",\n",
        "    \"Energy Breakdown: Modeled vs Expected\",\n",
        "    ax,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90204682",
      "metadata": {},
      "source": [
        "#### Energy Efficiency, Throughput, and Compute Density\n",
        "\n",
        "This test replicates the results of Table III in the paper.\n",
        "\n",
        "In this test, we show the energy efficiency, throughput, and compute density\n",
        "of the macro at 0.7V and 1.1V supply voltages.\n",
        "\n",
        "We see that increasing the supply voltage increases throughput at the cost of\n",
        "lower energy efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df3fd5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "spec = get_spec('wang_vlsi_2022', add_dummy_main_memory=True)\n",
        "spec.mapper.ffm.metrics = af.mapper.Metrics.ENERGY\n",
        "\n",
        "spec_evaluated = spec.calculate_component_area_energy_latency_leak()\n",
        "spec.variables.voltage = 0.7\n",
        "results_a = af.mapper.FFM.map_workload_to_arch(spec)\n",
        "spec.variables.voltage = 1.1\n",
        "results_b = af.mapper.FFM.map_workload_to_arch(spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef3fe67",
      "metadata": {},
      "outputs": [],
      "source": [
        "per_compute_latency_a = results_a.per_compute().to_dict()[\"Total<SEP>latency\"]\n",
        "per_compute_latency_b = results_b.per_compute().to_dict()[\"Total<SEP>latency\"]\n",
        "\n",
        "tops_a = 2 / results_a.per_compute().to_dict()[\"Total<SEP>latency\"] / 1e12\n",
        "tops_b = 2 / results_b.per_compute().to_dict()[\"Total<SEP>latency\"] / 1e12\n",
        "\n",
        "tops_per_mm_a = tops_a / total_area / 1e6\n",
        "tops_per_mm_b = tops_b / total_area / 1e6\n",
        "\n",
        "tops_per_w_a = 2 / results_a.per_compute().energy() / 1e12\n",
        "tops_per_w_b = 2 / results_b.per_compute().energy() / 1e12\n",
        "\n",
        "# Structure the dictionaries flat from the start\n",
        "modeled = {\n",
        "    \"0.7V tops_per_mm2\": tops_per_mm_a,\n",
        "    \"0.7V tops_per_w\": tops_per_w_a,\n",
        "    \"0.7V tops\": tops_a,\n",
        "    \"1.1V tops_per_mm2\": tops_per_mm_b,\n",
        "    \"1.1V tops_per_w\": tops_per_w_b,\n",
        "    \"1.1V tops\": tops_b,\n",
        "}\n",
        "\n",
        "expected = {\n",
        "    \"0.7V tops_per_mm2\": 2.4,\n",
        "    \"0.7V tops_per_w\": 32.2,\n",
        "    \"0.7V tops\": 0.3,\n",
        "    \"1.1V tops_per_mm2\": 4.0,\n",
        "    \"1.1V tops_per_w\": 15.5,\n",
        "    \"1.1V tops\": 0.5,\n",
        "}\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
        "ax.set_yscale('log')\n",
        "bar_comparison(\n",
        "    {\n",
        "        \"Modeled\": modeled,\n",
        "        \"Expected\": expected,\n",
        "    },\n",
        "    \"Metric\",\n",
        "    \"Value\",\n",
        "    \"Energy Efficiency, Throughput, and Compute Density\",\n",
        "    ax,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92021a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # def test_voltage_scaling():\n",
        "# #     \"\"\"\n",
        "# #     ### Voltage Scaling\n",
        "\n",
        "# #     This test replicates the results of Fig. 23 of the paper.\n",
        "\n",
        "# #     We show the effects of voltage scaling on the energy efficiency and\n",
        "# #     throughput of the macro, testing supply voltages of 0.7V, 0.8V, 0.9V, 1V,\n",
        "# #     and 1.1V.\n",
        "\n",
        "# #     We can see that increasing the supply voltage increases throughput and\n",
        "# #     compute density at the cost of lower energy efficiency.\n",
        "\n",
        "# #     Modeled and reference compute density varies because we did not model the\n",
        "# #     area of some miscellaneous components, leading to the model having a smaller\n",
        "# #     area and higher compute density. This could be corrected by adding\n",
        "# #     additional components to the model. We also use a different scaling factor\n",
        "# #     for voltage versus energy, leading to a different curve shape. This could be\n",
        "# #     corrected by adjusting the VOLTAGE_ENERGY_SCALE formula and propagating the\n",
        "# #     value to each subcomponent model.\n",
        "# #     \"\"\"\n",
        "# #     results = utl.parallel_test(\n",
        "# #         utl.delayed(utl.quick_run)(macro=MACRO_NAME, variables=dict(VOLTAGE=x))\n",
        "# #         for x in [0.7, 0.8, 0.9, 1, 1.1]\n",
        "# #     )\n",
        "\n",
        "# #     for r, tops_mm, tops_w in zip(\n",
        "# #         results,\n",
        "# #         [2.377, 2.858, 3.200, 3.596, 3.941],\n",
        "# #         [31.998, 22.590, 18.439, 16.376, 15.467],\n",
        "# #     ):\n",
        "# #         r.add_compare_ref(\"tops_per_mm2\", tops_mm)\n",
        "# #         r.add_compare_ref(\"tops_per_w\", tops_w)\n",
        "# #         r.add_compare_ref(\"tops\", tops_mm * TOTAL_AREA / 1e6)\n",
        "# #     return results\n",
        "\n",
        "\n",
        "# # def test_tops():\n",
        "# #     \"\"\"\n",
        "# #     ### Energy Efficiency, Throughput, and Compute Density\n",
        "\n",
        "# #     This test replicates the results of Table III in the paper.\n",
        "\n",
        "# #     In this test, we show the energy efficiency, throughput, and compute density\n",
        "# #     of the macro at 0.7V and 1.1V supply voltages.\n",
        "\n",
        "# #     We see that increasing the supply voltage increases throughput at the cost of\n",
        "# #     lower energy efficiency.\n",
        "# #     \"\"\"\n",
        "# #     results = utl.parallel_test(\n",
        "# #         utl.delayed(utl.quick_run)(macro=MACRO_NAME, variables=dict(VOLTAGE=x))\n",
        "# #         for x in [0.7, 1.1]\n",
        "# #     )\n",
        "# #     for r, tops_mm, tops_w, tops in zip(\n",
        "# #         results,\n",
        "# #         [2.4, 4.0],\n",
        "# #         [32.2, 15.5],\n",
        "# #         [0.3, 0.5],\n",
        "# #     ):\n",
        "# #         r.add_compare_ref(\"tops_per_mm2\", tops_mm)\n",
        "# #         r.add_compare_ref(\"tops_per_w\", tops_w)\n",
        "# #         r.add_compare_ref(\"tops\", tops)\n",
        "# #     return results\n",
        "\n",
        "\n",
        "# # def test_full_system_dnn(dnn_name: str, batch_size: int = None):\n",
        "# #     \"\"\"\n",
        "# #     ### Exploration of Full-System Energy Efficiency\n",
        "\n",
        "# #     In this test, we look at the full-system energy breakdown when running DNNs\n",
        "# #     on a CiM accelerator. We place the macro in a chip with local input/output\n",
        "# #     buffers, routers for on-chip data movement, a global buffer, and DRAM. We\n",
        "# #     show the area and energy spent on DRAM, the global buffer, and other\n",
        "# #     components.\n",
        "\n",
        "# #     We compare three scenarios:\n",
        "\n",
        "# #     1. Inputs, outputs, and weights stored off-chip in DRAM and fetched for each\n",
        "# #        layer\n",
        "# #     2. Inputs and outputs fetched from DRAM, weights stationary (pre-loaded for\n",
        "# #        each layer)\n",
        "# #     3. Weights stationary, layers fused to keep inputs/outputs on-chip in the\n",
        "# #        global\n",
        "\n",
        "# #     We can see that weight-stationary processing significantly reduces overall\n",
        "# #     energy due to fewer weight fetches from off-chip. Benefits are limited,\n",
        "# #     however, because inputs and outputs still must be fetched from off-chip. To\n",
        "# #     see further benefits, fusing layers is necessary to keep data on-chip\n",
        "# #     between DNN layers. We note that weight-stationary CiM requires sufficient\n",
        "# #     memory to keep all DNN weights on-chip. To store large DNNs, this may\n",
        "# #     require a multi-chip pipeline or dense storage technologies.\n",
        "# #     \"\"\"\n",
        "\n",
        "# #     dnn_dir = utl.path_from_model_dir(f\"workloads/{dnn_name}\")\n",
        "# #     layer_paths = [\n",
        "# #         os.path.join(dnn_dir, l) for l in os.listdir(dnn_dir) if l.endswith(\".yaml\")\n",
        "# #     ]\n",
        "\n",
        "# #     layer_paths = [l for l in layer_paths if \"From einsum\" not in open(l, \"r\").read()]\n",
        "\n",
        "# #     if \"gpt2_medium\" in dnn_name:\n",
        "# #         layer_paths = layer_paths[:-1]\n",
        "\n",
        "# #     def callfunc(spec):\n",
        "# #         spec.architecture.find(\"shared_router_group\").spatial.meshX = 64\n",
        "# #         spec.architecture.find(\"shared_router_group\").attributes.has_power_gating = True\n",
        "# #         spec.architecture.find(\"shared_router_group\").constraints.spatial.no_reuse = []\n",
        "\n",
        "# #         spec.architecture.find(\"tile_in_chip\").spatial.meshX = 16\n",
        "# #         spec.architecture.find(\"tile_in_chip\").attributes.has_power_gating = True\n",
        "# #         spec.architecture.find(\"tile_in_chip\").constraints.spatial.no_reuse = []\n",
        "\n",
        "# #         if batch_size is not None:\n",
        "# #             spec.problem.instance[\"N\"] = batch_size\n",
        "# #         spec.architecture.find(\"output_buffer\").constraints.temporal.iter_only = []\n",
        "\n",
        "# #     results = utl.parallel_test(\n",
        "# #         utl.delayed(utl.run_layer)(\n",
        "# #             macro=MACRO_NAME,\n",
        "# #             layer=l,\n",
        "# #             variables=dict(EXPERIMENT_NAME=s),\n",
        "# #             tile=\"input_output_bufs\",\n",
        "# #             chip=\"large_router_glb\",\n",
        "# #             system=system,\n",
        "# #             callfunc=callfunc,\n",
        "# #         )\n",
        "# #         for l in layer_paths\n",
        "# #         for s, system in (\n",
        "# #             ('\"All Tensors Off-Chip\"', \"fetch_all_lpddr4\"),\n",
        "# #             ('\"Weight-Stationary\"', \"fetch_weights_lpddr4\"),\n",
        "# #             ('\"Weight-Stationary + Fusion\"', None),\n",
        "# #         )\n",
        "# #     )\n",
        "\n",
        "# #     for r in results:\n",
        "# #         r.per_component_energy.setdefault(\"main_memory\", 0)\n",
        "\n",
        "# #     results.combine_per_component_energy(\n",
        "# #         [\n",
        "# #             \"c2c_multiplier_analog_port\",\n",
        "# #             \"c2c_multiplier_digital_port\",\n",
        "# #             \"cim_unit\",\n",
        "# #             \"adc\",\n",
        "# #             \"select_wordline_drivers\",\n",
        "# #             \"row_drivers\",\n",
        "# #             \"dac\",\n",
        "# #             \"output_buffer\",\n",
        "# #             \"input_buffer\",\n",
        "# #             \"router\",\n",
        "# #             \"weight_drivers\",\n",
        "# #             \"column_drivers\",\n",
        "# #         ],\n",
        "# #         \"Macro & Other On-Chip Data Movement\",\n",
        "# #     )\n",
        "# #     results.combine_per_component_energy([\"glb\"], \"Global Buffer\")\n",
        "# #     results.combine_per_component_energy([\"main_memory\"], \"Off-Chip DRAM\")\n",
        "# #     results.clear_zero_energies()\n",
        "\n",
        "# #     return results\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     test_energy_breakdown(),\n",
        "#     test_area_breakdown(),\n",
        "#     test_voltage_scaling(),\n",
        "#     test_tops(),\n",
        "#     test_full_system_dnn(\"resnet18\")\n",
        "\n",
        "\n",
        "\n",
        "# def test_tops():\n",
        "#     \"\"\"\n",
        "#     \"\"\"\n",
        "#     results = utl.parallel_test(\n",
        "#         utl.delayed(utl.quick_run)(macro=MACRO_NAME, variables=dict(VOLTAGE=x))\n",
        "#         for x in [0.7, 1.1]\n",
        "#     )\n",
        "#     for r, tops_mm, tops_w, tops in zip(\n",
        "#         results,\n",
        "#         [2.4, 4.0],\n",
        "#         [32.2, 15.5],\n",
        "#         [0.3, 0.5],\n",
        "#     ):\n",
        "#         r.add_compare_ref(\"tops_per_mm2\", tops_mm)\n",
        "#         r.add_compare_ref(\"tops_per_w\", tops_w)\n",
        "#         r.add_compare_ref(\"tops\", tops)\n",
        "#     return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# result = run_test(\"wang_vlsi_2022\", \"test_tops\")\n",
        "\n",
        "# fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "# bar_side_by_side(\n",
        "#     {r.variables[\"VOLTAGE\"]: r.tops for r in result},\n",
        "#     xlabel=\"Voltage (V)\",\n",
        "#     ylabel=\"Throughput (TOPS)\",\n",
        "#     title=\"Voltage vs. Throughput\",\n",
        "#     ax=ax[0],\n",
        "# )\n",
        "# bar_side_by_side(\n",
        "#     {r.variables[\"VOLTAGE\"]: r.tops_per_w for r in result},\n",
        "#     xlabel=\"Voltage (V)\",\n",
        "#     ylabel=\"Energy Efficiency (TOPS/W)\",\n",
        "#     title=\"Voltage vs. Energy Efficiency\",\n",
        "#     ax=ax[1],\n",
        "# )\n",
        "# bar_side_by_side(\n",
        "#     {r.variables[\"VOLTAGE\"]: r.tops_per_mm2 for r in result},\n",
        "#     xlabel=\"Voltage (V)\",\n",
        "#     ylabel=\"Compute Density (TOPS/mm^2)\",\n",
        "#     title=\"Voltage vs. Compute Density\",\n",
        "#     ax=ax[2],\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
