{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model of \"ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars!\", ISCA 2016\n",
    "\n",
    "Paper by Ali Shafiee, Anirban Nag, Naveen Muralimanohar, Rajeev Balasubramonian,\n",
    "John Paul Strachan, Miao Hu, R. Stanley Williams, and Vivek Srikumar.\n",
    "\n",
    "ISAAC is a ReRAM-based analog CiM accelerator. It explores several concepts in\n",
    "CiM acceleration, including storing different layers in different arrays and\n",
    "pipelining inputs/outputs between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Loading configuration file from /home/tanner/.config/accelforge/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Some of the important variables for isaac:\n",
       "\n",
       "- *array_wordlines*: array_parallel_inputs * cim_unit_width_cells rows in the array\n",
       "- *array_bitlines*: array_parallel_outputs * cim_unit_depth_cells columns in the array\n",
       "- *array_parallel_inputs*: get_array_fanout_reuse_output(spec) input slice(s) consumed in each cycle.\n",
       "- *array_parallel_weights*: get_array_fanout_total(spec) weights slice(s) used for computation in each cycle.\n",
       "- *array_parallel_outputs*: get_array_fanout_reuse_input(spec) partial sums produced in each cycle.\n",
       "- *tech_node*: 3.2e-08 m\n",
       "- *adc_resolution*: 8 bit(s)\n",
       "- *dac_resolution*: max(voltage_dac_resolution, temporal_dac_resolution) bit(s)\n",
       "- *n_adc_per_bank*: 1 ADC(s)\n",
       "- *supported_input_bits*: 8 bit(s)\n",
       "- *supported_output_bits*: 8 bit(s)\n",
       "- *supported_weight_bits*: 8 bit(s)\n",
       "- *bits_per_cell*: 2 bit(s)\n",
       "- *cim_unit_width_cells*: 1 adjacent cell(s) in a wordline store bit(s) in one weight slice and process one input & output slice together\n",
       "- *cim_unit_depth_cells*: 1 adjacent cell(s) in a bitline operate in separate cycles\n",
       "- *cell_config*: /home/tanner/research/fusion/fastfusion/fastfusion/examples/arches/compute_in_memory/memory_cells/rram_isaac_isca_2016.yaml \n",
       "- *cycle_period*: 1e-09 second(s)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Loading configuration file from /home/tanner/.config/accelforge/config.yaml\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"681pt\" height=\"440pt\"\n",
       " viewBox=\"0.00 0.00 681.00 440.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 436)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-436 677,-436 677,4 -4,4\"/>\n",
       "<!-- Memory_140457444474736 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Memory_140457444474736</title>\n",
       "<path fill=\"#d7fcd7\" stroke=\"black\" d=\"M597.5,-428.73C597.5,-430.53 480.52,-432 336.5,-432 192.48,-432 75.5,-430.53 75.5,-428.73 75.5,-428.73 75.5,-399.27 75.5,-399.27 75.5,-397.47 192.48,-396 336.5,-396 480.52,-396 597.5,-397.47 597.5,-399.27 597.5,-399.27 597.5,-428.73 597.5,-428.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M597.5,-428.73C597.5,-426.92 480.52,-425.45 336.5,-425.45 192.48,-425.45 75.5,-426.92 75.5,-428.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-410.9\" font-family=\"Arial\" font-size=\"12.00\">InputBuffer with size MultiArrayFanout.get_fanout() * array_parallel_inputs * supported_input_bits</text>\n",
       "</g>\n",
       "<!-- Memory_140457445450416 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Memory_140457445450416</title>\n",
       "<path fill=\"#d7fcd7\" stroke=\"black\" d=\"M673,-384.73C673,-386.53 522.18,-388 336.5,-388 150.82,-388 0,-386.53 0,-384.73 0,-384.73 0,-355.27 0,-355.27 0,-353.47 150.82,-352 336.5,-352 522.18,-352 673,-353.47 673,-355.27 673,-355.27 673,-384.73 673,-384.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M673,-384.73C673,-382.92 522.18,-381.45 336.5,-381.45 150.82,-381.45 0,-382.92 0,-384.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-366.9\" font-family=\"Arial\" font-size=\"12.00\">OutputBuffer with size MultiArrayFanout.get_fanout() * array_parallel_outputs // min_weight_slices * supported_output_bits * 2</text>\n",
       "</g>\n",
       "<!-- Memory_140457444474736&#45;&#45;Memory_140457445450416 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Memory_140457444474736&#45;&#45;Memory_140457445450416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-395.92C336.5,-393.37 336.5,-390.75 336.5,-388.2\"/>\n",
       "</g>\n",
       "<!-- Toll_140457445457536 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Toll_140457445457536</title>\n",
       "<polygon fill=\"#ffcc99\" stroke=\"black\" points=\"345.5,-338 309.5,-338 309.5,-314 345.5,-314 345.5,-308 363.5,-326 345.5,-344 345.5,-338\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-322.9\" font-family=\"Arial\" font-size=\"12.00\">ShiftAdd</text>\n",
       "</g>\n",
       "<!-- Memory_140457445450416&#45;&#45;Toll_140457445457536 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Memory_140457445450416&#45;&#45;Toll_140457445457536</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-351.92C336.5,-349.37 336.5,-346.75 336.5,-344.2\"/>\n",
       "</g>\n",
       "<!-- Fanout_140457445457616 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Fanout_140457445457616</title>\n",
       "<polygon fill=\"#fcc2fc\" stroke=\"black\" points=\"386.5,-300 286.5,-300 286.5,-264 386.5,-264 386.5,-300\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-285.4\" font-family=\"Arial\" font-size=\"12.00\">MultiArrayFanout</text>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-272.4\" font-family=\"Arial\" font-size=\"12.00\">[8× array]</text>\n",
       "</g>\n",
       "<!-- Toll_140457445457536&#45;&#45;Fanout_140457445457616 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Toll_140457445457536&#45;&#45;Fanout_140457445457616</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-307.92C336.5,-305.37 336.5,-302.75 336.5,-300.2\"/>\n",
       "</g>\n",
       "<!-- Toll_140457443607728 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Toll_140457443607728</title>\n",
       "<polygon fill=\"#ffcc99\" stroke=\"black\" points=\"345.5,-250 309.5,-250 309.5,-226 345.5,-226 345.5,-220 363.5,-238 345.5,-256 345.5,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-234.9\" font-family=\"Arial\" font-size=\"12.00\">ADC</text>\n",
       "</g>\n",
       "<!-- Fanout_140457445457616&#45;&#45;Toll_140457443607728 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Fanout_140457445457616&#45;&#45;Toll_140457443607728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-263.92C336.5,-261.37 336.5,-258.75 336.5,-256.2\"/>\n",
       "</g>\n",
       "<!-- Toll_140457443608128 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Toll_140457443608128</title>\n",
       "<polygon fill=\"#ffcc99\" stroke=\"black\" points=\"354.5,-206 300.5,-206 300.5,-182 354.5,-182 354.5,-176 372.5,-194 354.5,-212 354.5,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-190.9\" font-family=\"Arial\" font-size=\"12.00\">RowDrivers</text>\n",
       "</g>\n",
       "<!-- Toll_140457443607728&#45;&#45;Toll_140457443608128 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Toll_140457443607728&#45;&#45;Toll_140457443608128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-219.92C336.5,-217.37 336.5,-214.75 336.5,-212.2\"/>\n",
       "</g>\n",
       "<!-- Toll_140457443609328 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>Toll_140457443609328</title>\n",
       "<polygon fill=\"#ffcc99\" stroke=\"black\" points=\"363,-162 292,-162 292,-138 363,-138 363,-132 381,-150 363,-168 363,-162\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-146.9\" font-family=\"Arial\" font-size=\"12.00\">ColumnDrivers</text>\n",
       "</g>\n",
       "<!-- Toll_140457443608128&#45;&#45;Toll_140457443609328 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Toll_140457443608128&#45;&#45;Toll_140457443609328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-175.92C336.5,-173.37 336.5,-170.75 336.5,-168.2\"/>\n",
       "</g>\n",
       "<!-- Fanout_140457443611008 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>Fanout_140457443611008</title>\n",
       "<polygon fill=\"#fcc2fc\" stroke=\"black\" points=\"477,-124 196,-124 196,-88 477,-88 477,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-109.4\" font-family=\"Arial\" font-size=\"12.00\">ArrayFanout</text>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-96.4\" font-family=\"Arial\" font-size=\"12.00\">[128× array_reuse_input, 128× array_reuse_output]</text>\n",
       "</g>\n",
       "<!-- Toll_140457443609328&#45;&#45;Fanout_140457443611008 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>Toll_140457443609328&#45;&#45;Fanout_140457443611008</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-131.92C336.5,-129.37 336.5,-126.75 336.5,-124.2\"/>\n",
       "</g>\n",
       "<!-- Memory_140457443612288 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>Memory_140457443612288</title>\n",
       "<path fill=\"#d7fcd7\" stroke=\"black\" d=\"M594.5,-76.73C594.5,-78.53 478.86,-80 336.5,-80 194.14,-80 78.5,-78.53 78.5,-76.73 78.5,-76.73 78.5,-47.27 78.5,-47.27 78.5,-45.47 194.14,-44 336.5,-44 478.86,-44 594.5,-45.47 594.5,-47.27 594.5,-47.27 594.5,-76.73 594.5,-76.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M594.5,-76.73C594.5,-74.92 478.86,-73.45 336.5,-73.45 194.14,-73.45 78.5,-74.92 78.5,-76.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-58.9\" font-family=\"Arial\" font-size=\"12.00\">CimUnit with size cim_unit_width_cells * cim_unit_depth_cells * bits_per_cell * n_weight_slices</text>\n",
       "</g>\n",
       "<!-- Fanout_140457443611008&#45;&#45;Memory_140457443612288 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>Fanout_140457443611008&#45;&#45;Memory_140457443612288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-87.92C336.5,-85.37 336.5,-82.75 336.5,-80.2\"/>\n",
       "</g>\n",
       "<!-- Compute_140457443611808 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>Compute_140457443611808</title>\n",
       "<ellipse fill=\"#e0eeff\" stroke=\"black\" cx=\"336.5\" cy=\"-18\" rx=\"50.46\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-14.9\" font-family=\"Arial\" font-size=\"12.00\">FreeCompute</text>\n",
       "</g>\n",
       "<!-- Memory_140457443612288&#45;&#45;Compute_140457443611808 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>Memory_140457443612288&#45;&#45;Compute_140457443611808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.5,-43.92C336.5,-41.37 336.5,-38.75 336.5,-36.2\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "Arch(nodes=ArchNodes([Memory(name='InputBuffer', spatial=[], component_class='SmartBufferSRAM', component_model=None, component_modeling_log=[], actions=[TensorHolderAction(name='read', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action'), TensorHolderAction(name='write', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action')], enabled=True, area=None, total_area=None, area_scale=1, leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale=1, total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances=1, extra_attributes_for_component_model=_ExtraAttrs(), tensors=Tensors(keep='input', may_keep='<Nothing if keep is defined, else All>', back='Nothing', tile_shape=[], no_refetch_from_above='~All', tensor_order_options=[], force_memory_hierarchy_order=True), bits_per_value_scale={'All': 1}, bits_per_action=None, size='MultiArrayFanout.get_fanout() * array_parallel_inputs * supported_input_bits'), Memory(name='OutputBuffer', spatial=[], component_class='SmartBufferSRAM', component_model=None, component_modeling_log=[], actions=[TensorHolderAction(name='read', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action'), TensorHolderAction(name='write', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action')], enabled=True, area=None, total_area=None, area_scale=1, leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale=1, total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances=1, extra_attributes_for_component_model=_ExtraAttrs(), tensors=Tensors(keep='output', may_keep='<Nothing if keep is defined, else All>', back='Nothing', tile_shape=[], no_refetch_from_above='~All', tensor_order_options=[], force_memory_hierarchy_order=True), bits_per_value_scale={'All': 1}, bits_per_action=None, size='MultiArrayFanout.get_fanout() * array_parallel_outputs // min_weight_slices * supported_output_bits * 2'), Toll(name='ShiftAdd', spatial=[], component_class='ISAACShiftAdd', component_model=None, component_modeling_log=[], actions=[TensorHolderAction(name='read', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action')], enabled=True, area=None, total_area=None, area_scale=1, leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale=1, total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances='MultiArrayFanout.get_fanout()', extra_attributes_for_component_model=_ExtraAttrs(n_bits='supported_output_bits', shift_register_n_bits='supported_output_bits * 2'), tensors=Tensors(keep='output', may_keep='<Nothing if keep is defined, else All>', back='Nothing', tile_shape=[], no_refetch_from_above='~All', tensor_order_options=[], force_memory_hierarchy_order=True), bits_per_value_scale={'All': 1}, bits_per_action='output_bits / n_sliced_psums', direction='up'), Fanout(name='MultiArrayFanout', spatial=[Spatial(name='array', fanout=8, may_reuse='All', loop_bounds=[], min_usage=0.0, reuse='Nothing', usage_scale=1, power_gateable=False)]), Toll(name='ADC', spatial=[], component_class='ADC', component_model=None, component_modeling_log=[], actions=[TensorHolderAction(name='read', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action')], enabled=True, area=None, total_area=None, area_scale='adc_area_scale', leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale='adc_energy_scale', total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances=1, extra_attributes_for_component_model=_ExtraAttrs(throughput_scale='1 / 100', throughput='1 / cycle_period * cols_active_at_once * throughput_scale', n_bits='adc_resolution'), tensors=Tensors(keep='output', may_keep='<Nothing if keep is defined, else All>', back='Nothing', tile_shape=[], no_refetch_from_above='~All', tensor_order_options=[], force_memory_hierarchy_order=True), bits_per_value_scale={'All': 1}, bits_per_action='output_bits / n_sliced_psums', direction='up'), Toll(name='RowDrivers', spatial=[], component_class='ArrayRowDrivers', component_model=None, component_modeling_log=[], actions=[TensorHolderAction(name='read', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action')], enabled=True, area=None, total_area=None, area_scale=1, leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale=1, total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances=1, extra_attributes_for_component_model=_ExtraAttrs(), tensors=Tensors(keep='input', may_keep='<Nothing if keep is defined, else All>', back='Nothing', tile_shape=[], no_refetch_from_above='~All', tensor_order_options=[], force_memory_hierarchy_order=True), bits_per_value_scale={'All': 1}, bits_per_action='input_bits / n_input_slices', direction='down'), Toll(name='ColumnDrivers', spatial=[], component_class='ArrayColumnDrivers', component_model=None, component_modeling_log=[], actions=[TensorHolderAction(name='read', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action')], enabled=True, area=None, total_area=None, area_scale=1, leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale=1, total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances=1, extra_attributes_for_component_model=_ExtraAttrs(), tensors=Tensors(keep='output', may_keep='<Nothing if keep is defined, else All>', back='Nothing', tile_shape=[], no_refetch_from_above='~All', tensor_order_options=[], force_memory_hierarchy_order=True), bits_per_value_scale={'All': 1}, bits_per_action='output_bits / n_sliced_psums', direction='up'), Fanout(name='ArrayFanout', spatial=[Spatial(name='array_reuse_input', fanout=128, may_reuse='input', loop_bounds=[], min_usage=1, reuse='input', usage_scale='n_weight_slices', power_gateable=False), Spatial(name='array_reuse_output', fanout=128, may_reuse='output', loop_bounds=[], min_usage=1, reuse='output', usage_scale=1, power_gateable=False)]), Memory(name='CimUnit', spatial=[], component_class='MemoryCell', component_model=None, component_modeling_log=[], actions=[TensorHolderAction(name='read', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action'), TensorHolderAction(name='write', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras(), bits_per_action='1 if bits_per_action is None else bits_per_action')], enabled=True, area=None, total_area=None, area_scale=1, leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale=1, total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances='n_weight_slices', extra_attributes_for_component_model=_ExtraAttrs(n_instances='cim_unit_width_cells * cim_unit_depth_cells'), tensors=Tensors(keep='weight', may_keep='<Nothing if keep is defined, else All>', back='Nothing', tile_shape=[], no_refetch_from_above='weight', tensor_order_options=[], force_memory_hierarchy_order=False), bits_per_value_scale={'All': 1}, bits_per_action='weight.bits_per_value / n_sliced_psums', size='cim_unit_width_cells * cim_unit_depth_cells * bits_per_cell * n_weight_slices'), Compute(name='FreeCompute', spatial=[], component_class='Dummy', component_model=None, component_modeling_log=[], actions=[Action(name='compute', energy=None, energy_scale=1, latency=None, latency_scale=1, extra_attributes_for_component_model=EvalExtras())], enabled='len(All) == 3', area=None, total_area=None, area_scale=1, leak_power=None, total_leak_power=None, leak_power_scale=1, energy_scale=1, total_latency='sum(*action2latency.values())', latency_scale=1, n_parallel_instances=1, extra_attributes_for_component_model=_ExtraAttrs())]), variables=EvalExtras(encoded_input_bits='input_bits', encoded_weight_bits='weight_bits', encoded_output_bits='output_bits', input_encoding_func='offset_encode_hist', weight_encoding_func='offset_encode_hist', signed_sum_across_inputs=False, signed_sum_across_weights=False, cim_unit_width_cells=1, cim_unit_depth_cells=1, bits_per_cell=2, adc_resolution=8, voltage_dac_resolution=1, temporal_dac_resolution=1, n_adc_per_bank=1, cycle_period=1e-09, read_pulse_width=1e-09, weight_bits='weight.bits_per_value', input_bits='input.bits_per_value', output_bits='output.bits_per_value', array_parallel_inputs='get_array_fanout_reuse_output(spec)', array_parallel_outputs='get_array_fanout_reuse_input(spec)', array_parallel_weights='get_array_fanout_total(spec)', array_wordlines='array_parallel_inputs * cim_unit_width_cells', array_bitlines='array_parallel_outputs * cim_unit_depth_cells', dac_resolution='max(voltage_dac_resolution, temporal_dac_resolution)', cols_active_at_once='array_parallel_outputs', in_b='encoded_input_bits', w_b='encoded_weight_bits', max_input_bits_per_slice='min(dac_resolution, in_b)', max_weight_bits_per_slice='min(cim_unit_width_cells * bits_per_cell, w_b)', average_input_bits_per_slice='encoded_input_bits / n_input_slices', average_weight_bits_per_slice='encoded_weight_bits / n_weight_slices', n_virtual_macs='max_input_bits_per_slice * max_weight_bits_per_slice * encoded_output_bits', ehtas='encoded_hist_to_avg_slice', in_enc_fn='input_encoding_func', w_enc_fn='weight_encoding_func', average_input_value='ehtas(in_enc_fn(inputs_hist), in_b, max_input_bits_per_slice)', average_weight_value='ehtas(w_enc_fn(weights_hist), w_b, max_weight_bits_per_slice)', input_bit_distribution='ehtas(in_enc_fn(inputs_hist), in_b, 1, return_per_slice=True)', weight_bit_distribution='ehtas(w_enc_fn(weights_hist), w_b, 1, return_per_slice=True)', min_weight_slices='ceil(min_supported_weight_bits / bits_per_cell / cim_unit_width_cells)', min_input_slices='ceil(min_supported_input_bits / max_input_bits_per_slice)', n_input_slices='max(ceil(in_b / max_input_bits_per_slice), min_input_slices)', n_weight_slices='max(ceil(w_b / max_weight_bits_per_slice), min_weight_slices)', n_sliced_psums='n_input_slices * n_weight_slices'), extra_attributes_for_all_component_models=EvalExtras(tech_node='tech_node', cycle_period='cycle_period', rows='array_wordlines', cols='array_bitlines', cols_active_at_once='array_parallel_outputs', cell_config='cell_config', average_input_value='average_input_value', average_cell_value='average_weight_value', voltage='voltage', temporal_dac_bits='temporal_dac_resolution', read_pulse_width='read_pulse_width', resolution='adc_resolution', n_adcs='n_adc_per_bank', width='encoded_output_bits'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from _scripts import (\n",
    "    display_important_variables,\n",
    "    get_spec,\n",
    "    bar_comparison,\n",
    "    bar_stacked,\n",
    "    bar,\n",
    ")\n",
    "display_important_variables('isaac')\n",
    "get_spec('isaac').arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tile Level\n",
    "\n",
    "Twelve macros (called IMAs in the paper) are organized into a tile. Each tile\n",
    "includes a 64kB eDRAM buffer storing 16b inputs/outputs and quantization\n",
    "circuits. The original paper included sigmoid units at this level, but we\n",
    "replaced them with quantization circuits to match the other works. ISAAC uses\n",
    "16b fixed-point quantization for all operands.\n",
    "\n",
    "- *Input Path*: Inputs are stored in the eDRAM. An inter-macro network sends\n",
    "  inputs to macros in the tile.\n",
    "- *Weight Path*: Weights are kept static in inference and do not move through\n",
    "  this level.\n",
    "- *Output Path*: Outputs are gathered from macros via the inter-tile network.\n",
    "  They are quantized before being stored in the eDRAM.\n",
    "\n",
    "Next, there are 12 macros in each tile. Inputs and outputs are unicast between\n",
    "macros.\n",
    "\n",
    "### Macro Level\n",
    "\n",
    "Eight arrays are organized into a macro with an input register and output\n",
    "register. An input network sends input vectors to arrays.\n",
    "\n",
    "The eight arrays can process up to 8×128 = 1024 inputs across all rows, so the\n",
    "input register is sized 2kB (2B per input). The output register is sized 256B\n",
    "(2B per output, 128 outputs total (8 arrays × 128 columns × 2b per column / 16b\n",
    "per output)). While the paper does not do this, we double output buffer size to\n",
    "account for higher-precision accumulation that is important for lower-precision\n",
    "quantization.\n",
    "\n",
    "- *Input Path*: Inputs are stored in the input buffer and multicast between\n",
    "  arrays.\n",
    "- *Weight Path*: Weights are kept static in inference and do not move through\n",
    "  this level.\n",
    "- *Output Path*: Outputs are stored in the output buffer and spatially reduced\n",
    "  between arrays. Before the output buffer, a shift+add circuit accumulates\n",
    "  outputs and corrects for offsets caused by slicing.\n",
    "\n",
    "Next, there are 8 arrays in each macro. Inputs and outputs can be spatially\n",
    "reused across arrays with a multicast/reduction network.\n",
    "\n",
    "### Array Level\n",
    "\n",
    "Arrays consist of 128 × 128 ReRAMs. Each array is programmed with weights from\n",
    "one DNN layer, and each weight filter uses 8 array columns (16b weights, 2b per\n",
    "column). 1-bit DACs encode inputs across 16 cycles and 8-bit ADCs convert\n",
    "outputs from each column.\n",
    "\n",
    "We note that the original ISAAC paper included a contribution to decrease\n",
    "required ADC precision. Instead of supporting between 0 and the maximum output\n",
    "of a column, ISAAC supported only half of the range. They ensured that all\n",
    "column outputs would be in this range at program time. If the average weight\n",
    "slice value in a column was less than half of the maximum output, the column\n",
    "could not saturate the ADC. If the average weight slice value was greater than\n",
    "half of the maximum output, ISAAC would store the negated value of the weights.\n",
    "To correct for this, ISAAC would need to record sums of the input values, record\n",
    "which weight columns were negated, and perform arithmetic to recover the real\n",
    "sums from the negated sums.\n",
    "\n",
    "When we modeled ISAAC's accuracy, we found that this technique was not helpful\n",
    "across any tested workloads because weights tended to have about half of the\n",
    "maximum value and input bits tended to have >50% sparsity, so on average output\n",
    "of a column was around 25% of the output range anyway and never exceeded 50%. We\n",
    "can therefore just use the lower half of the ADC range to achieve the same\n",
    "result (lower ADC precision) without any of the additional complexity introduced\n",
    "by this strategy. For this reason, we don't model this technique in our ISAAC\n",
    "model.\n",
    "\n",
    "Inputs and weights are both assumed to be 16b unsigned fixed-point numbers.\n",
    "Signed inputs and weights are converted by adding a bias to the inputs and\n",
    "weights.\n",
    "\n",
    "- *Input Path*: Inputs pass through a 1-bit DACs and appear on the rows of the\n",
    "  array.\n",
    "- *Weight Path*: Weights are stored in the array and are not moved during\n",
    "  inference.\n",
    "- *Output Path*: Outputs are read from the columns of the array with 8-bit ADCs.\n",
    "\n",
    "Next, there are 128 columns in each array. Inputs are reused between columns\n",
    "(*i.e.,* each input-carrying wire connects to all columns), while outputs and\n",
    "weights are not reused.\n",
    "\n",
    "### Column Level\n",
    "\n",
    "Each column consists of 128 ReRAM devices. Columns store 2b weight slices.\n",
    "\n",
    "- *Input Path*: Each input is passed directly to a row in the column.\n",
    "- *Weight Path*: Weights are not moved during inference.\n",
    "- *Output Path*: Outputs pass through a current mirror to buffer their values\n",
    "  before exiting the column.\n",
    "\n",
    "### Row Level\n",
    "\n",
    "Each row in a column has one ReRAM device which stores an offset-encoded 2b\n",
    "weight slice.\n",
    "\n",
    "- *Input Path*: The input is used for a MAC operation.\n",
    "- *Weight Path*: A 2b weight is stored in the ReRAM device and is used for a MAC\n",
    "  operation.\n",
    "- *Output Path*: The output is supplied by a MAC operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelforge as af\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy Breakdown\n",
    "This test explores the energy, area, and latency of the accelerator\n",
    "computing MVM operations. We note a few differences from the original ISAAC\n",
    "paper. Notably, we made a few changes to the quantization, and we use\n",
    "data-value-dependent models while ISAAC used a simple fixed-power model.\n",
    "\n",
    "We note:\n",
    "- Energy is dominated by the ADC and memory cells due to the high ADC precision\n",
    "  and large number of slices.\n",
    "- Area is dominated by ADC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Loading configuration file from /home/tanner/.config/accelforge/config.yaml\n",
      "Getting energy, latency, and leak power for components running :   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting energy, latency, and leak power for components running Matmul: 100%|██████████| 1/1 [00:00<00:00, 15.96it/s]\n",
      "Generating pmapping templates for compute FreeCompute Einsum Matmul: 3it [00:00, 20.39it/s]\n",
      "Generating jobs: 100%|██████████| 1/1 [00:00<00:00,  5.98it/s]\n",
      "WARNING Einsum Matmul has 3 pmapping templates:\n",
      "WARNING \t[output,input in MainMemory] T-k  [input in InputBuffer] T-n  [output in OutputBuffer] T-k  [output in ShiftAdd] S-array-n  S-array-k  [output in ADC] [input in RowDrivers] [output in ColumnDrivers] S-array_reuse_output-k  S-array_reuse_input-n  [weight in CimUnit] T-k  T-m  T-n  FreeCompute computes Matmul\n",
      "WARNING \t[output,input in MainMemory] T-k  [input in InputBuffer] T-n  S-array-n  S-array-k  S-array_reuse_output-k  S-array_reuse_input-n  [weight in CimUnit] T-m  [output in OutputBuffer] [output in ShiftAdd] T-m  [output in ADC] [input in RowDrivers] [output in ColumnDrivers] T-k  T-m  T-n  FreeCompute computes Matmul\n",
      "WARNING \t[output,input in MainMemory] T-k  T-n  S-array-n  S-array-k  S-array_reuse_output-k  S-array_reuse_input-n  [weight in CimUnit] T-m  [input in InputBuffer] [output in OutputBuffer] [output in ShiftAdd] T-m  [output in ADC] [input in RowDrivers] [output in ColumnDrivers] T-k  T-m  T-n  FreeCompute computes Matmul\n",
      "WARNING Insufficient jobs available to utilize available threads. Splitting jobs into smaller chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not tracking memory MainMemory. It is big enough to simultaneously hold every workload tensor that may be stored in it. Max possible usage: 0.00%\n",
      "Not tracking memory InputBuffer across joining stages. It is never reserved across fused loop iterations.\n",
      "Not tracking memory OutputBuffer across joining stages. It is never reserved across fused loop iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pmappings: 100%|██████████| 3/3 [00:05<00:00,  1.77s/it]\n",
      "Grouping pmappings for Matmul: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matmul: 6.84e04 total, 20 (1/3.42e03) valid, 368 (1/186) evaluated, 1 (1/6.84e04) Pareto-Optimal\n",
      "Total: 6.84e04 total, 20 (1/3.42e03) valid, 368 (1/186) evaluated, 1 (1/6.84e04) Pareto-Optimal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing pmappings: 100%|██████████| 1/1 [00:00<00:00, 846.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not tracking CimUnit because it is never reserved for multiple pmappings.\n",
      "Not tracking CimUnit because its size is enough for the sum of all reservations (100.00% of the total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing unneeded reservations for Matmul: 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]\n",
      "Final consolidate: 100%|██████████| 2/2 [00:00<00:00,  3.37it/s]\n",
      "Grouping pmappings: 100%|██████████| 1/1 [00:00<00:00, 938.53it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'n_iterations<SEP>0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22113/2869481613.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'isaac'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_dummy_main_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFFM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENERGY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFFM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_workload_to_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_component\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/fusion/fastfusion/fastfusion/accelforge/mapper/FFM/main.py\u001b[0m in \u001b[0;36mmap_workload_to_arch\u001b[0;34m(spec, einsum_names, can_combine_multiple_runs, cache_dir, print_number_of_pmappings, _pmapping_row_filter_function)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlocal_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmappings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Total<SEP>mapping\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# BUG: Mapping._from_pmappings create mappings that cannot be evaluated!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         this_mapping = evaluate_mapping(\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mlocal_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mflattened_arches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmappings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflattened_arches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/fusion/fastfusion/fastfusion/accelforge/model/main.py\u001b[0m in \u001b[0;36mevaluate_mapping\u001b[0;34m(spec, flattened_arches, evaluated_specs)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msymbol_renames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'n_iterations<SEP>0'"
     ]
    }
   ],
   "source": [
    "spec = get_spec('isaac', add_dummy_main_memory=True)\n",
    "spec.mapper.ffm.metrics = af.mapper.FFM.Metrics.ENERGY\n",
    "results = spec.map_workload_to_arch()\n",
    "energy = results.per_compute().energy(per_component=True)\n",
    "\n",
    "spec_energy_area = spec.calculate_component_area_energy_latency_leak()\n",
    "area = spec_energy_area.arch.per_component_total_area\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "bar(\n",
    "    energy,\n",
    "    \"Component\",\n",
    "    \"Energy/Compute (J)\",\n",
    "    \"Energy Breakdown\",\n",
    "    ax,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "bar(\n",
    "    area,\n",
    "    \"Component\",\n",
    "    \"Area (m^2)\",\n",
    "    \"Area Breakdown\",\n",
    "    ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # fmt: oaf\n",
    "# THIS_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# MACRO_NAME = os.path.basename(THIS_SCRIPT_DIR)\n",
    "# sys.path.append(os.path.abspath(os.path.join(THIS_SCRIPT_DIR, '..', '..', '..', '..')))\n",
    "# from scripts import utils as utl\n",
    "# import scripts\n",
    "# # fmt: on\n",
    "\n",
    "# def test_full_dnn(dnn_name: str):\n",
    "#     \"\"\"\n",
    "#     This test explores the energy, area, and latency of the accelerator when\n",
    "#     running full DNN workloads.\n",
    "#     \"\"\"\n",
    "#     dnn_dir = utl.path_from_model_dir(f\"workloads/{dnn_name}\")\n",
    "#     layer_paths = [\n",
    "#         os.path.join(dnn_dir, l) for l in os.listdir(dnn_dir) if l.endswith(\".yaml\")\n",
    "#     ]\n",
    "\n",
    "#     layer_paths = [l for l in layer_paths if \"From einsum\" not in open(l, \"r\").read()]\n",
    "\n",
    "#     results = utl.parallel_test(\n",
    "#         utl.delayed(utl.run_layer)(\n",
    "#             macro=MACRO_NAME,\n",
    "#             layer=l,\n",
    "#             tile=\"isaac\",\n",
    "#             chip=\"large_router\",\n",
    "#         )\n",
    "#         for l in layer_paths\n",
    "#     )\n",
    "#     results.clear_zero_energies()\n",
    "#     return results\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_energy_breakdown()\n",
    "#     test_full_dnn(\"resnet18\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
