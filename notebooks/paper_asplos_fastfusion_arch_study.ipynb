{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from hwcomponents_cacti import SRAM as CactiSRAM\n",
    "from hwcomponents_library import AladdinAdder, AladdinMultiplier\n",
    "\n",
    "from fastfusion.frontend.arch import Memory\n",
    "from fastfusion.frontend.specification import Specification\n",
    "from fastfusion.mapper.FFM._make_pmappings.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.simanneal.wrappers import join_sims\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from fastfusion import Specification\n",
    "from fastfusion.mapper import Metrics\n",
    "from fastfusion.mapper.FFM._make_pmappings.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.FFM._join_pmappings.sim import SIM\n",
    "from fastfusion.mapper.FFM._join_pmappings.join_pmappings import join_sims\n",
    "import fastfusion.mapper.FFM._make_pmappings.mapper_one_einsum as mapper_one_einsum\n",
    "\n",
    "from fastfusion.mapper.FFM._pmapping_group import PmappingGroup\n",
    "from fastfusion.mapper.FFM import make_pmappings, join_pmappings\n",
    "\n",
    "# TODO: Make a setting for the below two in the spec\n",
    "# TODO: Generate pmappings one Einsum at a time. Once we've made compatibility, check it\n",
    "# against the previously-generated compatibilities and stop if there's no match.\n",
    "# TODO: Once the previous is done, also add a forward check. Once the compatibilities of\n",
    "# a particular Einsum are generated, we can immediately check the previous Einsums.\n",
    "\n",
    "objective = lambda df: df['Total\\0latency']# * df['Total_Energy']\n",
    "LOAD_FROM_CACHE = True\n",
    "\n",
    "def get_fused_mappings(\n",
    "        spec: Specification, \n",
    "        cache_key=None,\n",
    "        parameterization=\"\",\n",
    "    ) -> PmappingGroup:\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    if cache_key is not None:\n",
    "        fname = parameterization + \"-\".join(str(x) for x in cache_key)\n",
    "        if LOAD_FROM_CACHE and os.path.exists(f\"cache/{fname}.pkl\"):\n",
    "            print(f\"Loading from cache: {fname}\")\n",
    "            mappings = pickle.load(open(f\"cache/{fname}.pkl\", \"rb\"))\n",
    "            return objective(mappings.data).min() if mappings is not None else None, mappings\n",
    "    spec = copy.deepcopy(spec)\n",
    "    \n",
    "    main_memory: Memory = spec.arch.nodes[\"MainMemory\"]\n",
    "    if parameterization == \"Unfused\":\n",
    "        main_memory.constraints.tensors.keep = \"All()\"\n",
    "    elif parameterization == \"FlashAttention B\":\n",
    "        main_memory.constraints.tensors.keep = \"~bypass\"\n",
    "        main_memory.constraints.tensors.bypass = \"I | Q | K | V | QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FlashAttention A\":\n",
    "        main_memory.constraints.tensors.keep = \"~bypass\"\n",
    "        main_memory.constraints.tensors.bypass = \"QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FFM\":\n",
    "        main_memory.constraints.tensors.keep = \"~Intermediates()\" #\"# | AV | Z \"\n",
    "        pass\n",
    "    elif parameterization == \"Fixed-Dataflow\":\n",
    "        main_memory.constraints.tensors.keep = \"~Intermediates() | weight\"\n",
    "        spec.arch.nodes[\"GlobalBuffer\"].constraints.dataflow.tensor_order_options = [\n",
    "            [\"MainMemory.tensors() & weight\", \"MainMemory.tensors() & input\", \"MainMemory.tensors() & output\", \"weight - MainMemory.tensors()\", \"input - MainMemory.tensors()\", \"output - MainMemory.tensors()\"],\n",
    "        ]\n",
    "    else:\n",
    "        assert False, f\"Parameterization {parameterization} not supported\"\n",
    "    \n",
    "    if LOAD_FROM_CACHE and cache_key is not None and os.path.exists(f\"pmappings_cache/{fname}.pkl\"):\n",
    "        print(f\"Loading from cache: {fname}\")\n",
    "        pmappings = pickle.load(open(f\"cache/pmappings_{fname}.pkl\", \"rb\"))\n",
    "    else:\n",
    "        pmappings = make_pmappings(spec)\n",
    "        pickle.dump(pmappings, open(f\"cache/pmappings_{fname}.pkl\", \"wb\"))\n",
    "    try:\n",
    "        mappings = join_pmappings(spec, pmappings)\n",
    "    except:\n",
    "        mappings = None\n",
    "\n",
    "    # TODO: the final joined pmappings have lambdas somewhere, which can't be pickled.\n",
    "    if cache_key is not None:\n",
    "        pickle.dump(mappings, open(f\"cache/{fname}.pkl\", \"wb\"))\n",
    "        \n",
    "    return objective(mappings.data).min() if mappings is not None else None, mappings\n",
    "\n",
    "parameterization2edp = {}\n",
    "parameterization2mappings = {}\n",
    "\n",
    "parameterizations = [\"Unfused\", \"FlashAttention A\", \"FlashAttention B\", \"Fixed-Dataflow\", \"FFM\"]\n",
    "# parameterizations = [\"Unfused\", \"FFM\"] # \"FlashAttention A\", \"FlashAttention B\", \"Fixed-Dataflow\", \"FFM\"]\n",
    "# for batch_size, n_tokens in [(64, 512), (1, 8192), (1, 16384), (1, 32768), (64, 8192), (64, 16384), (64, 32768)]:\n",
    "for batch_size, n_tokens in [(64, 512), (1, 8192), (1, 32768)]:\n",
    "        for n_pes in [256]:# [64, 256]:\n",
    "            spec = Specification.from_yaml(\n",
    "                f\"architecture/tpu_like_asplos.arch.yaml\",\n",
    "                \"workloads/mha_full.workload.yaml\",\n",
    "                # \"workloads/matmuls8_mixed.workload.yaml\",\n",
    "                jinja_parse_data={\n",
    "                    \"BATCH_SIZE\": batch_size,\n",
    "                    \"N_TOKENS\": n_tokens,\n",
    "                    \"N_PES\": n_pes,\n",
    "                }\n",
    "            )\n",
    "            spec.mapper.ffm.metrics = Metrics.LATENCY\n",
    "            cache_key = (batch_size, n_tokens, n_pes)\n",
    "            spec.arch.nodes[\"LocalBuffer\"].spatial[\"Z\"].fanout = n_pes\n",
    "            for parameterization in parameterizations:\n",
    "                x, mappings = get_fused_mappings(\n",
    "                    spec,\n",
    "                    cache_key=cache_key,\n",
    "                    parameterization=parameterization,\n",
    "                )\n",
    "                parameterization2edp.setdefault((batch_size, n_tokens, n_pes), {})[parameterization] = x\n",
    "                parameterization2mappings.setdefault((batch_size, n_tokens, n_pes), {})[parameterization] = mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = parameterization2edp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "def plot_default_formatting(ax, grid_axis='both'):\n",
    "    ax.tick_params(axis='both', which='major')#, labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor')#, labelsize=20)\n",
    "    legend = ax.legend()\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('black')\n",
    "    # Set legend ncols to 5\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "    ax.legend(fontsize=14, ncols=5)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(axis=grid_axis, which='major', linestyle='-', linewidth='0.3', color='gray')\n",
    "    ax.grid(axis=grid_axis, which='minor', linestyle='--', linewidth='0.1', color='lightgray')\n",
    "\n",
    "colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#9467bd\",\n",
    "    \"#ff0000\",\n",
    "]\n",
    "\n",
    "def make_bar_chart(\n",
    "    data,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    y_scale,\n",
    "    output_file=None,\n",
    "    normalize: bool = False,\n",
    "    ylim=(None, None),\n",
    "    xlim=(None, None),\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a bar chart from the given data and save it as a PDF.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    if isinstance(data, dict) and isinstance(next(iter(data.values())), dict):\n",
    "        bar_width = 0.8 / len(data)\n",
    "        keys = list(next(iter(data.values())).keys())\n",
    "        x = range(len(keys))\n",
    "        first = next(iter(data.values()))\n",
    "            \n",
    "        for i, (label, values) in enumerate(data.items()):\n",
    "            bar_positions = [pos + i * bar_width for pos in x]\n",
    "            to_plot = values\n",
    "            if normalize:\n",
    "                to_plot = {k: v / first[k] for k, v in values.items()}\n",
    "            bars = plt.bar(bar_positions, to_plot.values(), width=bar_width, label=label, color=colors[i])\n",
    "        plt.xticks([pos + (len(data) - 1) * bar_width / 2 for pos in x], keys)\n",
    "        # plt.legend(loc='upper right', fontsize=10)\n",
    "        plt.legend(fontsize=10, ncol=len(data), loc='upper center')\n",
    "    else:\n",
    "        keys = list(data.keys())\n",
    "        bars = plt.bar(keys, data.values())\n",
    "        \n",
    "\n",
    "    # Set logarithmic scale for Y-axis if specified\n",
    "    if y_scale == 'log':\n",
    "        plt.yscale('log')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    # Rotate X-axis labels vertically\n",
    "    # plt.xticks(rotation=90)\n",
    "    \n",
    "    plot_default_formatting(plt.gca(), grid_axis='y')\n",
    "    \n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            plt.savefig(f, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "entries = {}\n",
    "\n",
    "name_changes = {\n",
    "    \"Unfused\": \"Elementwise-Only\",\n",
    "    \"FlashAttention A\": \"FlashAttention A\",\n",
    "    \"FlashAttention B\": \"FlashAttention B\",\n",
    "    \"FFM\": \"Fast & Fusiest\",\n",
    "    # (64, 512, 64): \"Big Batch\\n64 Cores\",\n",
    "    (64, 512, 256): \"Batch=64\\nTokens=512\",#\\n256 Cores\",\n",
    "    # (1, 16384, 64): \"Big Seq\\n64 Cores\",\n",
    "    (1, 8192, 256): \"Batch=1\\nTokens=8k\",#\\n256 Cores\",\n",
    "    # (1, 32768, 256): \"Bigger Seq\\n256 Cores\",\n",
    "    (1, 32768, 256): \"Batch=1\\nTokens=32k\",#\\n256 Cores\",\n",
    "}\n",
    "\n",
    "for k, v in results.items():\n",
    "    if k not in name_changes:\n",
    "        continue\n",
    "    k = name_changes.get(k, k)\n",
    "    entries[k] = {name_changes.get(k2, k2): 1/v[k2] if v[k2] else 0 for k2 in v}\n",
    "    max_val = max(entries[k].values())\n",
    "    for k2, v2 in entries[k].items():\n",
    "        entries[k][k2] = v2 / max_val if max_val else 0\n",
    "        \n",
    "entries={k: v for k, v in sorted(entries.items(), key=lambda x: list(name_changes.values()).index(x[0]))}\n",
    "\n",
    "# Transpose everything\n",
    "entries2 = {}\n",
    "for k, v in entries.items():\n",
    "    for k2, v2 in v.items():\n",
    "        entries2.setdefault(k2, {})[k] = v2\n",
    "entries = entries2\n",
    "        \n",
    "# Print as a table\n",
    "for name, e in entries2.items():\n",
    "    print(f\"{name}: {e}\")\n",
    "    \n",
    "make_bar_chart(entries, title=None, xlabel=None, ylabel=\"Throughput (normalized)\", y_scale='linear', output_file=\"mapsapce_compare.pdf\", normalize=False, ylim=(0, 1.14), xlim=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_ffm = next(iter(parameterization2mappings.values()))[\"FFM\"]\n",
    "from IPython.display import SVG\n",
    "display(SVG(mapping_ffm.render()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
