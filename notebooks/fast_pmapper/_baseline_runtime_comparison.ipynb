{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.frontend.specification import Specification\n",
    "from fastfusion import Specification\n",
    "from fastfusion.mapper.FFM import make_pmappings, join_pmappings\n",
    "import fastfusion as ff\n",
    "import time\n",
    "from joblib import Memory\n",
    "import pandas as pd\n",
    "\n",
    "# Set logging to warning\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "CACHE_DIR = \".cache\"\n",
    "memory = Memory(CACHE_DIR, compress=True)\n",
    "\n",
    "CACHE = True\n",
    "\n",
    "def cache(f):\n",
    "    if CACHE:\n",
    "        return memory.cache(f)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff.set_n_parallel_jobs(32)\n",
    "\n",
    "# spec = Specification.from_yaml(\n",
    "#     \"../../examples/arches/tpu_v4i_like.arch.yaml\",\n",
    "#     \"../../examples/workloads/gpt3_6.7B.workload.yaml\",\n",
    "#     jinja_parse_data=dict(BATCH_SIZE=64, N_TOKENS=65536)\n",
    "# )\n",
    "# einsum_name = \"QK\"\n",
    "\n",
    "# spec.arch.nodes[\"LocalBuffer\"].constraints.tensors.keep = \"input | output\"\n",
    "# spec.arch.nodes[\"GlobalBuffer\"].constraints.tensors.keep = \"input | output\"\n",
    "# spec.arch.nodes[\"MainMemory\"].constraints.tensors.keep = \"All\"\n",
    "# spec.mapper.ffm.metrics = ff.Metrics.ENERGY | ff.Metrics.LATENCY\n",
    "\n",
    "# t0 = time.perf_counter()\n",
    "# pmappings = make_pmappings(spec, einsum_names=[einsum_name])\n",
    "# t1 = time.perf_counter()\n",
    "# print(f\"Time taken: {t1 - t0} seconds\")\n",
    "# data = pd.concat(x.mappings.data for x in pmappings.einsum2pmappings[einsum_name])\n",
    "# edps = data[\"Total<SEP>energy\"] * data[\"Total<SEP>latency\"]\n",
    "# best = data.iloc[edps.argmin()]\n",
    "# joined = join_pmappings(spec, pmappings, require_all_einsums=True)\n",
    "# joined[0].render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKLOADS = [\n",
    "    \"mha_qk\",\n",
    "    \"gemm\",\n",
    "]\n",
    "\n",
    "N_CPUS = 16\n",
    "\n",
    "@cache\n",
    "def run_ffm(workload_name):\n",
    "    ff.set_n_parallel_jobs(1)\n",
    "\n",
    "    if workload_name == \"mha_qk\":\n",
    "        spec = Specification.from_yaml(\n",
    "            \"../../examples/arches/tpu_v4i_like.arch.yaml\",\n",
    "            \"../../examples/workloads/gpt3_6.7B.workload.yaml\",\n",
    "            jinja_parse_data=dict(BATCH_SIZE=64, N_TOKENS=65536)\n",
    "        )\n",
    "        einsum_name = \"QK\"\n",
    "    elif workload_name == \"gemm\":\n",
    "        spec = Specification.from_yaml(\n",
    "            \"../../examples/arches/tpu_v4i_like.arch.yaml\",\n",
    "            \"../../examples/workloads/matmuls.workload.yaml\",\n",
    "            jinja_parse_data=dict(M=16384, KN=16384, N_EINSUMS=1)\n",
    "        )\n",
    "        einsum_name = \"Matmul0\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown workload: {workload_name}\")\n",
    "\n",
    "    spec.arch[\"ArrayDummy\"].constraints.spatial[\"reuse_input\"].min_utilization = 0\n",
    "    spec.arch[\"ArrayDummy\"].constraints.spatial[\"reuse_output\"].min_utilization = 0\n",
    "    spec.arch.nodes[\"LocalBuffer\"].constraints.tensors.keep = \"input | output\"\n",
    "    spec.arch.nodes[\"LocalBuffer\"].constraints.spatial[\"Z\"].reuse = \"All\"\n",
    "    spec.arch.nodes[\"GlobalBuffer\"].constraints.tensors.keep = \"input | output\"\n",
    "    spec.arch.nodes[\"MainMemory\"].constraints.tensors.keep = \"All\"\n",
    "    spec.mapper.ffm.metrics = ff.Metrics.ENERGY | ff.Metrics.LATENCY\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    pmappings = make_pmappings(spec, einsum_names=[einsum_name])\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Time taken: {t1 - t0} seconds\")\n",
    "    data = pd.concat(x.mappings.data for x in pmappings.einsum2pmappings[einsum_name])\n",
    "    edps = data[\"Total<SEP>energy\"] * data[\"Total<SEP>latency\"]\n",
    "    best = data.iloc[edps.argmin()]\n",
    "    return best[\"Total<SEP>energy\"], best[\"Total<SEP>latency\"], t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytimeloop.timeloopfe.v4 as tl\n",
    "import pytimeloop.timeloopfe.common.backend_calls as tl_backend_calls\n",
    "from paths import DATA_DIR, TIMELOOP_CONFIG_DIR, TIMELOOP_WORKLOAD_DIR\n",
    "import shutil\n",
    "\n",
    "@cache\n",
    "def run_timeloop(workload_name, time_limit, use_hint=False):\n",
    "    #AHAHHFHAFHDH\n",
    "    # Clear the output directory\n",
    "    out_dir = \"outputs/timeloop\" + (\"_hint\" if use_hint else \"\")\n",
    "    shutil.rmtree(out_dir, ignore_errors=True)\n",
    "    spec = tl.Specification.from_yaml_files(\n",
    "        TIMELOOP_CONFIG_DIR / (\"tpu_like\" + (\"_hint\" if use_hint else \"\") + \".yaml\"),\n",
    "        TIMELOOP_WORKLOAD_DIR / f\"{workload_name}.yaml\",\n",
    "        TIMELOOP_CONFIG_DIR / \"tpu_like.ert.yaml\"\n",
    "    )\n",
    "    spec.mapper.evaluated_size = 1000000000\n",
    "\n",
    "    if time_limit / N_CPUS > 3:\n",
    "        time_limit /= N_CPUS\n",
    "        spec.mapper.num_threads = N_CPUS\n",
    "\n",
    "    print(f'Running timeloop with {spec.mapper.num_threads} threads for {time_limit} seconds')\n",
    "    proc = tl.call_mapper(spec, output_dir=out_dir, return_proc=True, log_to=f\"{out_dir}/mapper.log\")\n",
    "    time.sleep(time_limit)\n",
    "    tl.call_stop(proc)\n",
    "    # time.sleep(1)\n",
    "    # tl.call_stop(proc)\n",
    "    proc_result = proc.wait()\n",
    "    proc_result = 0 # Was succeeding and returning nonzero for some reason\n",
    "    with open(f\"{out_dir}/timeloop-mapper.ART.yaml\", \"w\") as f:\n",
    "        f.write(\"ART: {version: 0.4, tables: [{name: x.x, area: 1}]}\")\n",
    "    with open(f\"{out_dir}/timeloop-mapper.ERT.yaml\", \"w\") as f:\n",
    "        f.write(\"ERT: {version: 0.4, tables: []}\")\n",
    "    result = tl_backend_calls._parse_output(spec, out_dir, proc_result, for_model=False)\n",
    "    return (\n",
    "        result.energy,\n",
    "        result.latency,\n",
    "        time_limit\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import (\n",
    "    DATA_DIR,\n",
    "    ZIGZAG_MAPPING_DIR,\n",
    "    ZIGZAG_ARCHITECTURE_DIR,\n",
    "    ZIGZAG_WORKLOAD_DIR,\n",
    ")\n",
    "import time\n",
    "import zigzag\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "import zigzag\n",
    "from zigzag import api\n",
    "import uuid\n",
    "\n",
    "@cache\n",
    "def get_loma_multiproc_speed(**kwargs):\n",
    "    zigzag.stages.mapping.temporal_mapping_generator_stage.N_JOBS = 1\n",
    "    _, _, t1 = run_loma_single(\n",
    "        lpf_limit=7,\n",
    "        cache_spoiler=uuid.uuid4(),\n",
    "        **kwargs\n",
    "    )\n",
    "    zigzag.stages.mapping.temporal_mapping_generator_stage.N_JOBS = N_CPUS\n",
    "    _, _, t0 = run_loma_single(\n",
    "        lpf_limit=7,\n",
    "        cache_spoiler=uuid.uuid4(),\n",
    "        **kwargs\n",
    "    )\n",
    "    return t1 / t0\n",
    "\n",
    "\n",
    "@cache\n",
    "def run_loma_single(workload_name, lpf_limit, use_hint=False, cache_spoiler=None):\n",
    "    start = time.time()\n",
    "    energy, latency, cmes = api.get_hardware_performance_zigzag(\n",
    "        workload=str(ZIGZAG_WORKLOAD_DIR / f\"{workload_name}.yaml\"),\n",
    "        accelerator=str(ZIGZAG_ARCHITECTURE_DIR / (f\"tpu_like\" + (\"_bypass_hint\" if use_hint else \"\") + \".yaml\")),\n",
    "        mapping=str(ZIGZAG_MAPPING_DIR / f\"tpu_{workload_name}.yaml\"),\n",
    "        opt=\"EDP\",\n",
    "        # dump_folder=dump_folder,\n",
    "        # pickle_filename=pickle_filename,\n",
    "        lpf_limit=lpf_limit,\n",
    "        enable_mix_spatial_mapping=True,\n",
    "    )\n",
    "    end = time.time()\n",
    "    return energy/1e12, latency/1.05e9, end - start\n",
    "\n",
    "\n",
    "@cache\n",
    "def run_loma(workload_name, time_limit):\n",
    "    n_lpf = 1\n",
    "    time_limit_scale = 1\n",
    "    energy, latency, duration = None, None, None\n",
    "    while True:\n",
    "        if n_lpf >= 7:\n",
    "            zigzag.stages.mapping.temporal_mapping_generator_stage.N_JOBS = N_CPUS\n",
    "            time_limit_scale = get_loma_multiproc_speed(\n",
    "                workload_name=workload_name,\n",
    "                use_hint=False,\n",
    "            )\n",
    "            print(f'LOMA multiproc speed is {time_limit_scale}')\n",
    "        else:\n",
    "            zigzag.stages.mapping.temporal_mapping_generator_stage.N_JOBS = 1\n",
    "            time_limit_scale = 1\n",
    "\n",
    "        try:\n",
    "            scaled_time_limit = time_limit / time_limit_scale\n",
    "            print(f'Time limit is {scaled_time_limit}. Running LOMA:')\n",
    "            cur_energy, cur_latency, cur_duration = func_timeout(\n",
    "                scaled_time_limit,\n",
    "                run_loma_single,\n",
    "                kwargs=dict(workload_name=workload_name, lpf_limit=n_lpf),\n",
    "            )\n",
    "            cur_duration *= time_limit_scale\n",
    "            if cur_duration > time_limit:\n",
    "                break\n",
    "            energy, latency, duration = cur_energy, cur_latency, cur_duration\n",
    "        except FunctionTimedOut:\n",
    "            break\n",
    "        n_lpf += 1\n",
    "    if energy is None:\n",
    "        raise ValueError(\"Loma timed out\")\n",
    "    return energy, latency, duration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=36.448038906, use_hint=False)\n",
      "Running timeloop with 1 threads for 36.448038906 seconds\n",
      "____________________________________________________run_timeloop - 38.1s, 0.6min\n",
      "With hint: False, Timeloop EDP is 37337707.59529264 (2399.08x). Latency is 4398.046511104 (268.80x). Energy is 8489.61180856637 (8.93x).\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=36.448038906, use_hint=True)\n",
      "Running timeloop with 1 threads for 36.448038906 seconds\n",
      "____________________________________________________run_timeloop - 37.4s, 0.6min\n",
      "With hint: True, Timeloop EDP is 47459.82306030916 (3.05x). Latency is 30.360914905 (1.86x). Energy is 1563.1881716612306 (1.64x).\n",
      "ZigZag EDP is 19941.064340677516 (1.28x). Latency is 16.425707058095238 (1.00x). Energy is 1214.0155836305246 (1.28x).\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=364.48038906, use_hint=False)\n",
      "Running timeloop with 16 threads for 22.78002431625 seconds\n",
      "____________________________________________________run_timeloop - 23.9s, 0.4min\n",
      "With hint: False, Timeloop EDP is 7203523.571000056 (462.85x). Latency is 1099.511627776 (67.20x). Energy is 6551.566521920955 (6.89x).\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=364.48038906, use_hint=True)\n",
      "Running timeloop with 16 threads for 22.78002431625 seconds\n",
      "____________________________________________________run_timeloop - 24.8s, 0.4min\n",
      "With hint: True, Timeloop EDP is 47467.73453688565 (3.05x). Latency is 30.596042997 (1.87x). Energy is 1551.4337766337937 (1.63x).\n",
      "ZigZag EDP is 19941.063787537492 (1.28x). Latency is 16.425707058095238 (1.00x). Energy is 1214.0155499552604 (1.28x).\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=3644.8038906, use_hint=False)\n",
      "Running timeloop with 16 threads for 227.8002431625 seconds\n",
      "___________________________________________________run_timeloop - 235.5s, 3.9min\n",
      "With hint: False, Timeloop EDP is 2982011.8636325886 (191.60x). Latency is 1099.511627776 (67.20x). Energy is 2712.123990597855 (2.85x).\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=3644.8038906, use_hint=True)\n",
      "Running timeloop with 16 threads for 227.8002431625 seconds\n",
      "___________________________________________________run_timeloop - 230.5s, 3.8min\n",
      "With hint: True, Timeloop EDP is 46746.416519446255 (3.00x). Latency is 30.360914905 (1.86x). Energy is 1539.6906406054252 (1.62x).\n",
      "ZigZag EDP is 19940.971942945504 (1.28x). Latency is 16.425748045714286 (1.00x). Energy is 1214.0069290876766 (1.28x).\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=36448.038906, use_hint=False)\n",
      "Running timeloop with 16 threads for 2278.002431625 seconds\n",
      "_________________________________________________run_timeloop - 2278.2s, 38.0min\n",
      "With hint: False, Timeloop EDP is 94673.83903384075 (6.08x). Latency is 41.382544226 (2.53x). Energy is 2287.77231570887 (2.41x).\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-1267128337.run_timeloop...\n",
      "run_timeloop('mha_qk', time_limit=36448.038906, use_hint=True)\n",
      "Running timeloop with 16 threads for 2278.002431625 seconds\n",
      "_________________________________________________run_timeloop - 2278.2s, 38.0min\n",
      "With hint: True, Timeloop EDP is 44782.79477266941 (2.88x). Latency is 30.184568835 (1.84x). Energy is 1483.6320842437308 (1.56x).\n",
      "ZigZag EDP is 18863.436977942903 (1.21x). Latency is 16.613633502857144 (1.02x). Energy is 1135.4191107381084 (1.19x).\n"
     ]
    }
   ],
   "source": [
    "runtime_limits = [1, 10, 100, 1000]\n",
    "results_edp = {runtime_limit: {} for runtime_limit in runtime_limits}\n",
    "results_latency = {runtime_limit: {} for runtime_limit in runtime_limits}\n",
    "results_energy = {runtime_limit: {} for runtime_limit in runtime_limits}\n",
    "workload_name = \"mha_qk\"\n",
    "\n",
    "ffm_energy, ffm_latency, ffm_duration = run_ffm(workload_name)\n",
    "ffm_edp = ffm_energy * ffm_latency\n",
    "\n",
    "def fmt(x, y):\n",
    "    return f\"{x} ({x / y:.2f}x)\"\n",
    "\n",
    "for runtime_limit in runtime_limits:\n",
    "    runtime_limit_seconds = ffm_duration * runtime_limit\n",
    "    # Timeloop\n",
    "    for use_hint in [False, True]:\n",
    "        while True:\n",
    "            energy, latency, duration = run_timeloop(workload_name, time_limit=runtime_limit_seconds, use_hint=use_hint)\n",
    "            edp = energy * latency\n",
    "            print(f\"With hint: {use_hint}, Timeloop EDP is {fmt(edp, ffm_edp)}. Latency is {fmt(latency, ffm_latency)}. Energy is {fmt(energy, ffm_energy)}.\")\n",
    "            results_edp[runtime_limit][\"Timeloop\" + (\" + Hint\" if use_hint else \"\")] = edp\n",
    "            results_latency[runtime_limit][\"Timeloop\" + (\" + Hint\" if use_hint else \"\")] = latency\n",
    "            results_energy[runtime_limit][\"Timeloop\" + (\" + Hint\" if use_hint else \"\")] = energy\n",
    "            break\n",
    "\n",
    "    # ZigZag\n",
    "    zigzag_energy, zigzag_latency, runtime = run_loma(workload_name, runtime_limit_seconds)\n",
    "    results_edp[runtime_limit][\"ZigZag\"] = zigzag_energy * zigzag_latency\n",
    "    results_latency[runtime_limit][\"ZigZag\"] = zigzag_latency\n",
    "    results_energy[runtime_limit][\"ZigZag\"] = zigzag_energy\n",
    "    print(f\"ZigZag EDP is {fmt(zigzag_energy * zigzag_latency, ffm_edp)}. Latency is {fmt(zigzag_latency, ffm_latency)}. Energy is {fmt(zigzag_energy, ffm_energy)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_table(results):\n",
    "    def floatfmt(x):\n",
    "        if isinstance(x, str):\n",
    "            return x\n",
    "        if x > 100:\n",
    "            return round(x)\n",
    "        if x == 1:\n",
    "            return int(x)\n",
    "        return f\"{x:.2f}\"\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(floatfmt)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    runtime_labels = [f\"$10^{{{x}}}$\" for x in df.index]\n",
    "\n",
    "    # Transpose it\n",
    "    df = df.T\n",
    "    df.columns = runtime_labels\n",
    "    df[\" \"] = cols # Put this at the beginning\n",
    "    df = df.reindex(columns=[\" \", *df.columns[:-1]])\n",
    "\n",
    "    # Add a row above the runtimes (all but the last columns, centered) that says\n",
    "\n",
    "    latex = df.to_latex(\n",
    "        index=5,\n",
    "        float_format=floatfmt,\n",
    "        column_format=\"l\" + \"c\" * (len(df.columns) - 1)\n",
    "    )\n",
    "\n",
    "    # Insert \"Runtime\" row above all runtime columns (centered)\n",
    "    # Identify the header row from pandas and inject our own.\n",
    "    lines = latex.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(' '):   # first header line\n",
    "            # Extract runtime columns (skip the first \" \")\n",
    "            runtimes = df.columns[1:]\n",
    "            num_runtime_cols = len(runtimes)\n",
    "\n",
    "            # Center \"Runtime\" across all runtime columns\n",
    "            # Use multicolumn spanning them\n",
    "            runtime_header = (\n",
    "                \" & \" +\n",
    "                f\"\\\\multicolumn{{{num_runtime_cols}}}{{c}}{{Best-Found EDP In X Runtime}} \\\\\\\\\"\n",
    "            )\n",
    "            # Replace header line with: empty cell + our multicolumn\n",
    "            lines.insert(i, \"  \" + runtime_header)\n",
    "            break\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"  & $10^{0}$\"):\n",
    "            lines[i] = line.replace(\"  & $10^{0}$\", \"Runtime & $10^{0}$\")\n",
    "            break\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"Turbo-Charged\"):\n",
    "            n_replacements = len(runtime_labels) - 1\n",
    "            to_replace = \" & 1\" * n_replacements + \" \\\\\\\\\"\n",
    "            replace_with = f\" & \\\\multicolumn{{{n_replacements}}}{{c}}{{Completed}} \\\\\\\\\"\n",
    "            lines[i] = lines[i].replace(to_replace, replace_with)\n",
    "            break\n",
    "\n",
    "    with open(\"outputs/results.tex\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "import copy\n",
    "results = copy.deepcopy(results_edp)\n",
    "for k, v in results.items():\n",
    "    for k2, v2 in v.items():\n",
    "        results[k][k2] /= ffm_edp\n",
    "\n",
    "get_latex_table(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
