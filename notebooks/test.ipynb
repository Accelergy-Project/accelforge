{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 19:38:47 INFO        Loading yaml file architecture/four_level.arch.yaml\n",
      "2025-05-23 19:38:47 INFO        Found top key variables in architecture/four_level.arch.yaml\n",
      "2025-05-23 19:38:47 INFO        Found top key architecture in architecture/four_level.arch.yaml\n",
      "2025-05-23 19:38:47 INFO        Found top key component_classes in architecture/four_level.arch.yaml\n",
      "2025-05-23 19:38:47 INFO        Loading yaml file workloads/mha_full_new.yaml\n",
      "2025-05-23 19:38:47 INFO        Found top key workload in workloads/mha_full_new.yaml\n",
      "2025-05-23 19:38:47 INFO        Loading plug-ins from /home/plug-ins/neurosim-plug-in/main.py. Errors below are likely due to the plug-in.\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimADC that estimates array_adc with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimAdder that estimates adder with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimAdderTree that estimates adder_tree with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimColDrivers that estimates array_col_drivers with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimFlipFlop that estimates flip_flop with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimMaxPool that estimates max_pool with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimMemoryCell that estimates memory_cell with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimMux that estimates mux with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimNANDGate that estimates nand_gate with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimNORGate that estimates nor_gate with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimNOTGate that estimates not_gate with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimPIMComponent that estimates _override_this_name_ with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimPlugInComponent that estimates _override_this_name_ with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimRowDrivers that estimates array_row_drivers with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NeurosimShiftAdd that estimates shift_add with actions add, compute, convert, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Loading plug-ins from /home/plug-ins/cacti-plug-in/main.py. Errors below are likely due to the plug-in.\n",
      "2025-05-23 19:38:47 INFO        Added estimator CactiCache that estimates cache with actions read, read_access, update, update_access, write, write_access, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator CactiDRAM that estimates ['DRAM', 'dram'] with actions read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator CactiSRAM that estimates ['SRAM', 'sram'] with actions read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Loading plug-ins from /home/plug-ins/adc-plug-in/main.py. Errors below are likely due to the plug-in.\n",
      "2025-05-23 19:38:47 INFO        Added estimator ADCEstimator that estimates ['adc', 'pim_adc', 'sar_adc', 'array_adc', 'pim_array_adc', 'cim_array_adc', 'cim_adc'] with actions activate, convert, drive, read, sample, leak\n",
      "2025-05-23 19:38:47 INFO        Loading plug-ins from /home/plug-ins/library-plug-in/main.py. Errors below are likely due to the plug-in.\n",
      "2025-05-23 19:38:47 INFO        Loaded 133 components from library.\n",
      "2025-05-23 19:38:47 INFO        Added estimator AladdinAdder that estimates aladdin_adder with actions add, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AladdinComparator that estimates aladdin_comparator with actions compare, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AladdinCounter that estimates aladdin_counter with actions count, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AladdinMultiplier that estimates aladdin_multiplier with actions leak, multiply, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AladdinRegister that estimates aladdin_register with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerAdc that estimates atomlayer_adc with actions convert, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerDac that estimates atomlayer_dac with actions drive, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerEdram that estimates atomlayer_edram with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerEdramBus that estimates atomlayer_edram_bus with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerInputBufferTransfers that estimates atomlayer_input_buffer_transfers with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerRegisterLadder that estimates atomlayer_register_ladder with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerRouter that estimates atomlayer_router with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator AtomlayerShiftAdd that estimates atomlayer_shift_add with actions leak, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator BrahmsDac that estimates brahms_dac with actions convert, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator DummyCompute that estimates dummy_compute with actions *, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator DummyStorage that estimates dummy_storage with actions *, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator FormsAdc that estimates forms_adc with actions convert, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator FormsDac that estimates forms_dac with actions drive, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacAdc that estimates isaac_adc with actions convert, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacChip2chipLink that estimates isaac_chip2chip_link with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacDac that estimates isaac_dac with actions drive, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacEdram that estimates isaac_edram with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacEdramBus that estimates isaac_edram_bus with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacRouter that estimates isaac_router with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacRouterSharedByFour that estimates isaac_router_shared_by_four with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator IsaacShiftAdd that estimates isaac_shift_add with actions leak, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator JiaDatapath that estimates jia_datapath with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator JiaShiftAdd that estimates jia_shift_add with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator JiaZeroGate that estimates jia_zero_gate with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator LibraryEstimatorClassBase that estimates _must_override_name_ with actions leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NewtonAdc that estimates newton_adc with actions convert, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NewtonDac that estimates newton_dac with actions drive, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NewtonEdram that estimates newton_edram with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NewtonEdramBus that estimates newton_edram_bus with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NewtonRouter that estimates newton_router with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator NewtonShiftAdd that estimates newton_shift_add with actions leak, read, shift_add, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator RaaamEdram that estimates raaam_edram with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator RaellaQuantMultiplier that estimates raella_quant_multiplier with actions leak, multiply, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyChargingComparator that estimates timely_charging_comparator with actions compare, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyChip2chipLink that estimates timely_chip2chip_link with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyDtc that estimates timely_dtc with actions convert, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyIadder that estimates timely_iadder with actions add, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyInputOutputBuffer that estimates timely_input_output_buffer with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyPsubbuf that estimates timely_psubbuf with actions convert, drive, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyTdc that estimates timely_tdc with actions convert, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator TimelyXsubbuf that estimates timely_xsubbuf with actions buffer, drive, leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator WanAnalogIntegrator that estimates wan_analog_integrator with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator WanAnalogSample that estimates wan_analog_sample with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator WanShiftAdd that estimates wan_shift_add with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Added estimator WanVariablePrecisionAdc that estimates wan_variable_precision_adc with actions leak, read, update, write, leak\n",
      "2025-05-23 19:38:47 INFO        Calculated \"(450 / 64) / 8\" = 0.87890625.\n",
      "2025-05-23 19:38:47 INFO        Calculated \"8 * width\" = 65536.\n",
      "2025-05-23 19:38:47 INFO        Calculated \"1024*1024*128*8\" = 1073741824.\n",
      "2025-05-23 19:38:47 INFO        Calculated \"1024*1024*32*8\" = 268435456.\n",
      "2025-05-23 19:38:47 INFO        \n",
      "2025-05-23 19:38:47 INFO        AREA ESTIMATION for DRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=0.87890625, area_scale=1, datawidth=8, size=9999999999999, multiple_buffering=1, version=0.5, width=4678)\n",
      "2025-05-23 19:38:47 INFO        CactiDRAM estimated 0 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:47 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['DRAM', 'dram'].__init__. Arguments used: (width)\n",
      "2025-05-23 19:38:47 INFO        | Multiplying by n_instances 1\n",
      "2025-05-23 19:38:47 INFO        \n",
      "2025-05-23 19:38:47 INFO        ENERGY ESTIMATION for DRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=0.87890625, area_scale=1, datawidth=8, size=9999999999999, multiple_buffering=1, version=0.5, width=4678).read(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:47 INFO        CactiDRAM estimated 3.7424e-08 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:47 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['DRAM', 'dram'].__init__. Arguments used: (width)\n",
      "2025-05-23 19:38:47 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for ['DRAM', 'dram'].read. Arguments used: ()\n",
      "2025-05-23 19:38:47 INFO        \n",
      "2025-05-23 19:38:47 INFO        ENERGY ESTIMATION for DRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=0.87890625, area_scale=1, datawidth=8, size=9999999999999, multiple_buffering=1, version=0.5, width=4678).write(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:47 INFO        CactiDRAM estimated 3.7424e-08 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:47 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['DRAM', 'dram'].__init__. Arguments used: (width)\n",
      "2025-05-23 19:38:47 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for ['DRAM', 'dram'].write. Arguments used: ()\n",
      "2025-05-23 19:38:47 INFO        \n",
      "2025-05-23 19:38:47 INFO        ENERGY ESTIMATION for DRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=0.87890625, area_scale=1, datawidth=8, size=9999999999999, multiple_buffering=1, version=0.5, width=4678).leak(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:47 INFO        CactiDRAM estimated 0 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:47 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['DRAM', 'dram'].__init__. Arguments used: (width)\n",
      "2025-05-23 19:38:47 INFO        | Unused arguments (technology, n_instances) provided for ['DRAM', 'dram'].leak. Arguments used: (global_cycle_seconds)\n",
      "2025-05-23 19:38:47 INFO        | Multiplying by n_instances 1\n",
      "2025-05-23 19:38:47 INFO        \n",
      "2025-05-23 19:38:47 INFO        AREA ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1, datawidth=65536, size=1073741824, multiple_buffering=1, version=0.5, width=8192, depth=1000)\n",
      "2025-05-23 19:38:48 INFO        CactiSRAM estimated 1.5183445454545452e-06 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:48 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with cache_size=1024000 n_rw_ports=1 block_size=1024 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmppteg8xs2\n",
      "2025-05-23 19:38:48 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpsne0n6j0\n",
      "2025-05-23 19:38:48 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmppteg8xs2 >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpsne0n6j0 2>&1\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1024.0 bytes/cycle\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1318714575240.9404 bits/second\n",
      "2025-05-23 19:38:48 INFO        | Multiplying by n_instances 1\n",
      "2025-05-23 19:38:48 INFO        \n",
      "2025-05-23 19:38:48 INFO        ENERGY ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1, datawidth=65536, size=1073741824, multiple_buffering=1, version=0.5, width=8192, depth=1000).read(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:48 INFO        CactiSRAM estimated 1.2838597093282287e-09 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:48 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:48 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for ['SRAM', 'sram'].read. Arguments used: ()\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with cache_size=1024000 n_rw_ports=1 block_size=1024 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp2k_fv4od\n",
      "2025-05-23 19:38:48 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp7qja77w7\n",
      "2025-05-23 19:38:48 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp2k_fv4od >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp7qja77w7 2>&1\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1024.0 bytes/cycle\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1318714575240.9404 bits/second\n",
      "2025-05-23 19:38:48 INFO        \n",
      "2025-05-23 19:38:48 INFO        ENERGY ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1, datawidth=65536, size=1073741824, multiple_buffering=1, version=0.5, width=8192, depth=1000).write(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:48 INFO        CactiSRAM estimated 1.5506564111955358e-09 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:48 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:48 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for ['SRAM', 'sram'].write. Arguments used: ()\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with cache_size=1024000 n_rw_ports=1 block_size=1024 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmppn_oc4ag\n",
      "2025-05-23 19:38:48 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmppptko4oj\n",
      "2025-05-23 19:38:48 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmppn_oc4ag >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmppptko4oj 2>&1\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1024.0 bytes/cycle\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1318714575240.9404 bits/second\n",
      "2025-05-23 19:38:48 INFO        \n",
      "2025-05-23 19:38:48 INFO        ENERGY ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1, datawidth=65536, size=1073741824, multiple_buffering=1, version=0.5, width=8192, depth=1000).leak(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:48 INFO        CactiSRAM estimated 6.855063108567962e-14 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:48 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:48 INFO        | Unused arguments (technology, n_instances) provided for ['SRAM', 'sram'].leak. Arguments used: (global_cycle_seconds)\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with cache_size=1024000 n_rw_ports=1 block_size=1024 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:48 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpb6pn31x2\n",
      "2025-05-23 19:38:48 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpz_vw4_u8\n",
      "2025-05-23 19:38:48 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpb6pn31x2 >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpz_vw4_u8 2>&1\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1024.0 bytes/cycle\n",
      "2025-05-23 19:38:48 INFO        | Cache bandwidth: 1318714575240.9404 bits/second\n",
      "2025-05-23 19:38:48 INFO        | Global cycle time 1e-09 is less than the cache's random cycle time 6.212110000000001e-09\n",
      "2025-05-23 19:38:48 INFO        | Multiplying by n_instances 1\n",
      "2025-05-23 19:38:48 INFO        \n",
      "2025-05-23 19:38:48 INFO        AREA ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=16, energy_scale=12, area_scale=1, datawidth=8, size=268435456, multiple_buffering=1, version=0.5, width=2048, depth=1000)\n",
      "2025-05-23 19:38:49 INFO        CactiSRAM estimated 1.2495381818181817e-06 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with cache_size=256000 n_rw_ports=1 block_size=256 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpqk1hkyvn\n",
      "2025-05-23 19:38:49 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpg4a2_pfi\n",
      "2025-05-23 19:38:49 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpqk1hkyvn >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpg4a2_pfi 2>&1\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 256.0 bytes/cycle\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 785547159679.3372 bits/second\n",
      "2025-05-23 19:38:49 INFO        | Multiplying by n_instances 16\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=16, energy_scale=12, area_scale=1, datawidth=8, size=268435456, multiple_buffering=1, version=0.5, width=2048, depth=1000).read(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        CactiSRAM estimated 6.91602393812567e-11 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for ['SRAM', 'sram'].read. Arguments used: ()\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with cache_size=256000 n_rw_ports=1 block_size=256 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp9gpl1yp7\n",
      "2025-05-23 19:38:49 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpe5fs2koe\n",
      "2025-05-23 19:38:49 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp9gpl1yp7 >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpe5fs2koe 2>&1\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 256.0 bytes/cycle\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 785547159679.3372 bits/second\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=16, energy_scale=12, area_scale=1, datawidth=8, size=268435456, multiple_buffering=1, version=0.5, width=2048, depth=1000).write(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        CactiSRAM estimated 1.2864036924256568e-10 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for ['SRAM', 'sram'].write. Arguments used: ()\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with cache_size=256000 n_rw_ports=1 block_size=256 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpl8dt1z81\n",
      "2025-05-23 19:38:49 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp7ij4ft4z\n",
      "2025-05-23 19:38:49 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpl8dt1z81 >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmp7ij4ft4z 2>&1\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 256.0 bytes/cycle\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 785547159679.3372 bits/second\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for SRAM(technology=7nm, global_cycle_seconds=1e-09, n_instances=16, energy_scale=12, area_scale=1, datawidth=8, size=268435456, multiple_buffering=1, version=0.5, width=2048, depth=1000).leak(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        CactiSRAM estimated 2.4820397827263454e-13 with accuracy 80. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (global_cycle_seconds, n_instances, datawidth, size, multiple_buffering, version) provided for ['SRAM', 'sram'].__init__. Arguments used: (technology, width, depth)\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, n_instances) provided for ['SRAM', 'sram'].leak. Arguments used: (global_cycle_seconds)\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with cache_size=256000 n_rw_ports=1 block_size=256 tech_node_um=0.022 n_banks=1 tag_size=0 associativity=1\n",
      "2025-05-23 19:38:49 INFO        | Calling CACTI with input path /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpq2bj70jy\n",
      "2025-05-23 19:38:49 INFO        | CACTI output will be written to /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpg_g8bsif\n",
      "2025-05-23 19:38:49 INFO        | Calling: cd /home/plug-ins/cacti-plug-in/cacti ; ./cacti -infile /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpq2bj70jy >> /home/plug-ins/cacti-plug-in/cacti_inputs_outputs/tmpg_g8bsif 2>&1\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 256.0 bytes/cycle\n",
      "2025-05-23 19:38:49 INFO        | Cache bandwidth: 785547159679.3372 bits/second\n",
      "2025-05-23 19:38:49 INFO        | Global cycle time 1e-09 is less than the cache's random cycle time 2.6071000000000002e-09\n",
      "2025-05-23 19:38:49 INFO        | Multiplying by n_instances 16\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        AREA ESTIMATION for dummy_storage(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=1, area_scale=1, datawidth=8, size=8, multiple_buffering=1, version=0.5, width=8, depth=1000)\n",
      "2025-05-23 19:38:49 INFO        DummyStorage estimated 0.0 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, datawidth, size, multiple_buffering, version, width, depth) provided for dummy_storage.__init__. Arguments used: (global_cycle_seconds, n_instances)\n",
      "2025-05-23 19:38:49 INFO        | Multiplying by n_instances 262144\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for dummy_storage(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=1, area_scale=1, datawidth=8, size=8, multiple_buffering=1, version=0.5, width=8, depth=1000).read(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        DummyStorage estimated 0.0 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, datawidth, size, multiple_buffering, version, width, depth) provided for dummy_storage.__init__. Arguments used: (global_cycle_seconds, n_instances)\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for dummy_storage.read. Arguments used: ()\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for dummy_storage(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=1, area_scale=1, datawidth=8, size=8, multiple_buffering=1, version=0.5, width=8, depth=1000).write(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        DummyStorage estimated 0.0 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, datawidth, size, multiple_buffering, version, width, depth) provided for dummy_storage.__init__. Arguments used: (global_cycle_seconds, n_instances)\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for dummy_storage.write. Arguments used: ()\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for dummy_storage(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=1, area_scale=1, datawidth=8, size=8, multiple_buffering=1, version=0.5, width=8, depth=1000).leak(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        DummyStorage estimated 0.0 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, datawidth, size, multiple_buffering, version, width, depth) provided for dummy_storage.__init__. Arguments used: (global_cycle_seconds, n_instances)\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for dummy_storage.leak. Arguments used: ()\n",
      "2025-05-23 19:38:49 INFO        | Multiplying by n_instances 262144\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        AREA ESTIMATION for aladdin_adder(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=0, area_scale=1, width=16)\n",
      "2025-05-23 19:38:49 INFO        AladdinAdder estimated 3.7419039507692303e-06 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Multiplying by n_instances 262144\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        AREA ESTIMATION for aladdin_multiplier(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=1, area_scale=1, width_a=8, width_b=8)\n",
      "2025-05-23 19:38:49 INFO        AladdinMultiplier estimated 1.0683943384615385e-05 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Multiplying by n_instances 262144\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for aladdin_adder(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=0, area_scale=1, width=16).read(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        AladdinAdder estimated 1.2528975135199513e-14 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for aladdin_adder.read. Arguments used: ()\n",
      "2025-05-23 19:38:49 INFO        \n",
      "2025-05-23 19:38:49 INFO        ENERGY ESTIMATION for aladdin_multiplier(technology=7nm, global_cycle_seconds=1e-09, n_instances=262144, energy_scale=1, area_scale=1, width_a=8, width_b=8).read(technology=7nm, global_cycle_seconds=1e-09, n_instances=1, energy_scale=1, area_scale=1)\n",
      "2025-05-23 19:38:49 INFO        AladdinMultiplier estimated 9.456393137757727e-14 with accuracy 90. Messages:\n",
      "2025-05-23 19:38:49 INFO        | Unused arguments (technology, global_cycle_seconds, n_instances) provided for aladdin_multiplier.read. Arguments used: ()\n"
     ]
    }
   ],
   "source": [
    "from fastfusion import Specification\n",
    "spec = Specification.from_yaml(\n",
    "    \"architecture/four_level.arch.yaml\",\n",
    "    \"workloads/mha_full_new.yaml\"\n",
    ")\n",
    "spec.estimate_energy_area()\n",
    "spec.component_energy.to_yaml(\"ERT.yaml\")\n",
    "spec.component_area.to_yaml(\"ART.yaml\")\n",
    "# spec.workload.mermaid_graph() # pip3 install mermaid-py\n",
    "# spec.workload.einsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 19:38:50 INFO        Loading yaml file workloads/mha_full_new.yaml\n",
      "2025-05-23 19:38:50 INFO        Found top key workload in workloads/mha_full_new.yaml\n",
      "2025-05-23 19:38:50 WARNING     Trying to parse a single element dictionary as a Workload. \n",
      "2025-05-23 19:38:50 INFO        Loading yaml file workloads/mha_full_new.renames.yaml\n",
      "2025-05-23 19:38:50 INFO        Found top key renames in workloads/mha_full_new.renames.yaml\n",
      "2025-05-23 19:38:50 WARNING     Trying to parse a single element dictionary as a Renames. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='V' tensor_accesses=[TensorAccess(name='I', projection={'B': 'b', 'M': 'm', 'D': 'd'}, output=False, factors=['b', 'm', 'd']), TensorAccess(name='WV', projection={'H': 'h', 'E': 'e', 'D': 'd'}, output=False, factors=['h', 'e', 'd']), TensorAccess(name='V', projection={'B': 'b', 'M': 'm', 'H': 'h', 'E': 'e'}, output=True, factors=['b', 'm', 'h', 'e'])] shape=[]\n",
      "Einsum V:\n",
      "\tAll: {I, V, WV}\n",
      "\tInputs: {I, WV}\n",
      "\tOutputs: {V}\n",
      "\tIntermediates: {V}\n",
      "\tShared: {I, V}\n",
      "\tWV: {WV}\n",
      "\tV: {V}\n",
      "\tI: {I}\n",
      "\tm: {m}\n",
      "\te: {e}\n",
      "\tb: {b}\n",
      "\th: {h}\n",
      "\td: {d}\n",
      "name='K' tensor_accesses=[TensorAccess(name='I', projection={'B': 'b', 'M': 'm', 'D': 'd'}, output=False, factors=['b', 'm', 'd']), TensorAccess(name='WK', projection={'H': 'h', 'E': 'e', 'D': 'd'}, output=False, factors=['h', 'e', 'd']), TensorAccess(name='K', projection={'B': 'b', 'M': 'm', 'H': 'h', 'E': 'e'}, output=True, factors=['b', 'm', 'h', 'e'])] shape=[]\n",
      "Einsum K:\n",
      "\tAll: {I, K, WK}\n",
      "\tInputs: {I, WK}\n",
      "\tOutputs: {K}\n",
      "\tIntermediates: {K}\n",
      "\tShared: {I, K}\n",
      "\tK: {K}\n",
      "\tWK: {WK}\n",
      "\tI: {I}\n",
      "\tm: {m}\n",
      "\te: {e}\n",
      "\tb: {b}\n",
      "\th: {h}\n",
      "\td: {d}\n",
      "name='Q' tensor_accesses=[TensorAccess(name='I', projection={'B': 'b', 'M': 'm', 'D': 'd'}, output=False, factors=['b', 'm', 'd']), TensorAccess(name='WQ', projection={'H': 'h', 'E': 'e', 'D': 'd'}, output=False, factors=['h', 'e', 'd']), TensorAccess(name='Q', projection={'B': 'b', 'M': 'm', 'H': 'h', 'E': 'e'}, output=True, factors=['b', 'm', 'h', 'e'])] shape=[]\n",
      "Einsum Q:\n",
      "\tAll: {I, Q, WQ}\n",
      "\tInputs: {I, WQ}\n",
      "\tOutputs: {Q}\n",
      "\tIntermediates: {Q}\n",
      "\tShared: {I, Q}\n",
      "\tQ: {Q}\n",
      "\tWQ: {WQ}\n",
      "\tI: {I}\n",
      "\tm: {m}\n",
      "\te: {e}\n",
      "\tb: {b}\n",
      "\th: {h}\n",
      "\td: {d}\n",
      "name='QK' tensor_accesses=[TensorAccess(name='Q', projection={'B': 'b', 'M': 'm', 'H': 'h', 'E': 'e'}, output=False, factors=['b', 'm', 'h', 'e']), TensorAccess(name='K', projection={'B': 'b', 'M': 'p', 'H': 'h', 'E': 'e'}, output=False, factors=['b', 'p', 'h', 'e']), TensorAccess(name='QK', projection={'B': 'b', 'M': 'm', 'P': 'p', 'H': 'h'}, output=True, factors=['b', 'm', 'p', 'h'])] shape=[]\n",
      "Einsum QK:\n",
      "\tAll: {K, Q, QK}\n",
      "\tInputs: {K, Q}\n",
      "\tOutputs: {QK}\n",
      "\tIntermediates: {K, Q, QK}\n",
      "\tShared: {K, Q, QK}\n",
      "\tK: {K}\n",
      "\tQ: {Q}\n",
      "\tQK: {QK}\n",
      "\tm: {m}\n",
      "\te: {e}\n",
      "\tb: {b}\n",
      "\th: {h}\n",
      "\tp: {p}\n",
      "name='AV' tensor_accesses=[TensorAccess(name='QK', projection={'B': 'b', 'M': 'm', 'P': 'p', 'H': 'h'}, output=False, factors=['b', 'm', 'p', 'h']), TensorAccess(name='V', projection={'B': 'b', 'M': 'p', 'H': 'h', 'E': 'f'}, output=False, factors=['b', 'p', 'h', 'f']), TensorAccess(name='AV', projection={'B': 'b', 'M': 'm', 'H': 'h', 'F': 'f'}, output=True, factors=['b', 'm', 'h', 'f'])] shape=[]\n",
      "Einsum AV:\n",
      "\tAll: {AV, QK, V}\n",
      "\tInputs: {QK, V}\n",
      "\tOutputs: {AV}\n",
      "\tIntermediates: {AV, QK, V}\n",
      "\tShared: {AV, QK, V}\n",
      "\tAV: {AV}\n",
      "\tV: {V}\n",
      "\tQK: {QK}\n",
      "\tm: {m}\n",
      "\tb: {b}\n",
      "\th: {h}\n",
      "\tf: {f}\n",
      "\tp: {p}\n",
      "name='Z' tensor_accesses=[TensorAccess(name='AV', projection={'B': 'b', 'M': 'm', 'H': 'h', 'F': 'F'}, output=False, factors=['b', 'm', 'h', 'F']), TensorAccess(name='WZ', projection={'H': 'h', 'F': 'F', 'G': 'g'}, output=False, factors=['h', 'F', 'g']), TensorAccess(name='Z', projection={'B': 'b', 'M': 'm', 'G': 'g'}, output=True, factors=['b', 'm', 'g'])] shape=[]\n",
      "Einsum Z:\n",
      "\tAll: {AV, WZ, Z}\n",
      "\tInputs: {AV, WZ}\n",
      "\tOutputs: {Z}\n",
      "\tIntermediates: {AV, Z}\n",
      "\tShared: {AV, Z}\n",
      "\tZ: {Z}\n",
      "\tWZ: {WZ}\n",
      "\tAV: {AV}\n",
      "\tm: {m}\n",
      "\tF: {F}\n",
      "\th: {h}\n",
      "\tb: {b}\n",
      "\tg: {g}\n",
      "name='FFA' tensor_accesses=[TensorAccess(name='Z', projection={'B': 'b', 'M': 'm', 'G': 'g'}, output=False, factors=['b', 'm', 'g']), TensorAccess(name='WFFA', projection={'G': 'g', 'C': 'c'}, output=False, factors=['g', 'c']), TensorAccess(name='FFA', projection={'B': 'b', 'M': 'm', 'C': 'c'}, output=True, factors=['b', 'm', 'c'])] shape=[]\n",
      "Einsum FFA:\n",
      "\tAll: {FFA, WFFA, Z}\n",
      "\tInputs: {WFFA, Z}\n",
      "\tOutputs: {FFA}\n",
      "\tIntermediates: {FFA, Z}\n",
      "\tShared: {FFA, Z}\n",
      "\tZ: {Z}\n",
      "\tWFFA: {WFFA}\n",
      "\tFFA: {FFA}\n",
      "\tm: {m}\n",
      "\tc: {c}\n",
      "\tb: {b}\n",
      "\tg: {g}\n",
      "name='FFB' tensor_accesses=[TensorAccess(name='FFA', projection={'B': 'b', 'M': 'm', 'C': 'c'}, output=False, factors=['b', 'm', 'c']), TensorAccess(name='WFFB', projection={'C': 'c', 'J': 'j'}, output=False, factors=['c', 'j']), TensorAccess(name='FFB', projection={'B': 'b', 'M': 'm', 'J': 'j'}, output=True, factors=['b', 'm', 'j'])] shape=[]\n",
      "Einsum FFB:\n",
      "\tAll: {FFA, FFB, WFFB}\n",
      "\tInputs: {FFA, WFFB}\n",
      "\tOutputs: {FFB}\n",
      "\tIntermediates: {FFA}\n",
      "\tShared: {FFA}\n",
      "\tFFB: {FFB}\n",
      "\tFFA: {FFA}\n",
      "\tWFFB: {WFFB}\n",
      "\tm: {m}\n",
      "\tc: {c}\n",
      "\tj: {j}\n",
      "\tb: {b}\n"
     ]
    }
   ],
   "source": [
    "from fastfusion.frontend.renames import Renames\n",
    "from fastfusion.frontend.workload import Workload\n",
    "workload = Workload.from_yaml(\n",
    "    \"workloads/mha_full_new.yaml\",\n",
    ")\n",
    "renames = Renames.from_yaml(\n",
    "    \"workloads/mha_full_new.renames.yaml\",\n",
    ")\n",
    "for e in workload.einsums:\n",
    "    print(e)\n",
    "    symbol_table = workload.get_constraint_symbol_table(e.name, renames)\n",
    "    print(f'Einsum {e.name}:')\n",
    "    for k, v in symbol_table.items():\n",
    "        print(f'\\t{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import itertools\n",
    "# from typing import Generator\n",
    "# from fastfusion.frontend._set_parse import InvertibleSet\n",
    "# from fastfusion.frontend.arch import Leaf, Storage\n",
    "# from fastfusion.frontend.workload.workload_spec import RankVariable, Tensor\n",
    "\n",
    "\n",
    "# einsum_name = \"Q\"\n",
    "# einsum = workload.einsums[einsum_name]\n",
    "# rank_variables = einsum.rank_variables\n",
    "# tensors = einsum.tensors\n",
    "# symbol_table = workload.get_constraint_symbol_table(einsum_name, renames)\n",
    "# first_value = next(iter(symbol_table.values()))\n",
    "# arch_nodes = spec.get_flattened_architecture()\n",
    "# tensor2rank_variables = einsum.tensor2rank_variables\n",
    "\n",
    "# # for node in arch_nodes:\n",
    "# #     storage_constraints = node.constraints.storage._parse(symbol_table)\n",
    "# #     print(f'{node.name}:')\n",
    "# #     for k, v in storage_constraints.items():\n",
    "# #         print(f'\\t{k}: {v}')\n",
    "\n",
    "# #     keep_choice = first_value.to_my_space(set((\"Q\", \"K\")))\n",
    "# #     symbol_table[node.name] = keep_choice\n",
    "\n",
    "# #     temporal = node.constraints.temporal._parse(symbol_table)\n",
    "# #     spatial_X = node.constraints.get_spatial_constraint(for_X=True)._parse(symbol_table)\n",
    "# #     spatial_Y = node.constraints.get_spatial_constraint(for_Y=True)._parse(symbol_table)\n",
    "\n",
    "# #     for k, v in temporal.items():\n",
    "# #         print(f'\\tTemporal {k}: {v}')\n",
    "# #     for k, v in spatial_X.items():\n",
    "# #         print(f'\\tSpatial X {k}: {v}')\n",
    "# #     for k, v in spatial_Y.items():\n",
    "# #         print(f'\\tSpatial Y {k}: {v}')\n",
    "        \n",
    "\n",
    "# from itertools import chain, combinations\n",
    "# def powerset(iterable):\n",
    "#     \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "#     s = list(iterable)\n",
    "#     return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "# def make_storage_choices_one_level(node: Leaf, symbol_table: dict[str, InvertibleSet]):\n",
    "#     if not isinstance(node, Storage):\n",
    "#         yield set(), symbol_table\n",
    "#         return\n",
    "#     new_symbol_table = copy.copy(symbol_table)\n",
    "#     storage_constraints = node.constraints.storage._parse(symbol_table)\n",
    "#     must_keep = first_value.to_my_space(storage_constraints[\"keep\"])\n",
    "#     must_bypass = first_value.to_my_space(storage_constraints[\"bypass\"])\n",
    "    \n",
    "#     if must_keep - new_symbol_table[\"All\"]:\n",
    "#         raise KeyError(f\"Keep constraint for {node.name} includes tensors that are not in the einsum: {must_keep - new_symbol_table['All']}\")\n",
    "#     if must_bypass - new_symbol_table[\"All\"]:\n",
    "#         raise KeyError(f\"Bypass constraint for {node.name} includes tensors that are not in the einsum: {must_bypass - tensors}\")\n",
    "#     if must_keep & must_bypass:\n",
    "#         raise KeyError(f\"Keep and bypass constraints for {node.name} intersect: {must_keep & must_bypass}\")\n",
    "    \n",
    "    \n",
    "#     may_keep = tensors - must_bypass - must_keep\n",
    "#     for subset in powerset(may_keep):\n",
    "#         subset = first_value.to_my_space(set(subset))\n",
    "#         keep_choice = first_value.to_my_space(subset | must_keep)\n",
    "#         keep_choice.tensors = lambda: keep_choice # So users can do MainMemory().tensors(). Optional.\n",
    "#         new_symbol_table[node.name] = keep_choice\n",
    "#         assert not any(isinstance(k, str) for k in keep_choice)\n",
    "#         yield keep_choice, new_symbol_table\n",
    "\n",
    "# def make_temporal_loop_choices(node: Storage, symbol_table: dict[str, InvertibleSet]) -> Generator[list[str], None, None]:\n",
    "#     if not isinstance(node, Storage):\n",
    "#         yield set()\n",
    "#         return\n",
    "    \n",
    "#     temporal_constraints = node.constraints.temporal._parse(symbol_table)\n",
    "#     for i in range(len(temporal_constraints.loop_order)):\n",
    "#         for j in range(i + 1, len(temporal_constraints.loop_order)):\n",
    "#             if temporal_constraints.loop_order[i] & temporal_constraints.loop_order[j]:\n",
    "#                 raise ValueError(\n",
    "#                     f\"Loop order constraint indices {i} and {j} for {node.name} intersect:\"\n",
    "#                     f\" {temporal_constraints.loop_order[i] & temporal_constraints.loop_order[j]}\"\n",
    "#                 )\n",
    "\n",
    "#     loop_order_constraint = temporal_constraints.loop_order\n",
    "#     loop_order_ranks = set.union(*(set(s) for s in loop_order_constraint), set())\n",
    "#     def is_valid_loop_order(loop_order):\n",
    "#         loop_order = [l for l in loop_order if l in loop_order_ranks]\n",
    "#         if not loop_order:\n",
    "#             return True\n",
    "\n",
    "#         i = 0\n",
    "#         for rank_variable in loop_order:\n",
    "#             if i >= len(loop_order_constraint):\n",
    "#                 return False\n",
    "#             while rank_variable not in loop_order_constraint[i]:\n",
    "#                 i += 1\n",
    "#                 if i >= len(loop_order_constraint):\n",
    "#                     return False\n",
    "#         return True\n",
    "    \n",
    "#     to_permute = rank_variables - temporal_constraints.loop_bounds.get_constrained_one_rank_variables()\n",
    "#     for loop_order in itertools.permutations(to_permute, len(to_permute)):\n",
    "#         if is_valid_loop_order(loop_order):\n",
    "#             yield loop_order\n",
    "            \n",
    "# def make_storage_choices_all_levels(nodes: list[Storage], symbol_table: dict[str, InvertibleSet]):\n",
    "#     while nodes and not isinstance(nodes[0], Storage):\n",
    "#         nodes = nodes[1:]\n",
    "#     if len(nodes) == 0:\n",
    "#         yield dict(), symbol_table\n",
    "#         return\n",
    "\n",
    "#     for choice, symbol_table in make_storage_choices_one_level(nodes[0], symbol_table):\n",
    "#         for subchoices, symbol_table in make_storage_choices_all_levels(nodes[1:], symbol_table):\n",
    "#             yield {**subchoices, nodes[0].name: choice}, symbol_table\n",
    "\n",
    "    \n",
    "# # UNEVEN = True        \n",
    "# # def uneven_storage(node: Leaf, symbol_table: dict[str, InvertibleSet], storage_choice: set[str], loop_order: list[str], previous_loop=None):\n",
    "# #     if not UNEVEN:\n",
    "# #         return {t: 0 for t in storage_choice}\n",
    "    \n",
    "# #     if previous_loop is None:\n",
    "# #         previous_loop = set()\n",
    "# #     for i, t in enumerate(loop_order):\n",
    "# #         if t in previous_loop:\n",
    "# #             continue\n",
    "# #         previous_loop.add(t)\n",
    "# #         yield from uneven_storage(node, symbol_table, storage_choice, loop_order, previous_loop)\n",
    "\n",
    "# def place_tensor(node: Storage, tensor: Tensor, mapping: list[str]) -> Generator[list[str], None, None]:\n",
    "#     if len([x for x in mapping if isinstance(x, RankVariable)]) <= 1:\n",
    "#         yield [tensor] + mapping\n",
    "#         return\n",
    "    \n",
    "#     last_mapping_appearance = max((i for i, t in enumerate(mapping) if t == tensor), default=None)\n",
    "#     appeared_before = True\n",
    "#     if last_mapping_appearance is None:\n",
    "#         appeared_before = False\n",
    "#         last_mapping_appearance = 0\n",
    "\n",
    "#     for i in range(last_mapping_appearance, len(mapping) - 1):\n",
    "#         # We're not worried about relative ordering of storage. Only storages vs. loops.\n",
    "#         if not isinstance(mapping[i], RankVariable):\n",
    "#             continue\n",
    "\n",
    "#         if not appeared_before:\n",
    "#             cur_irrelevant = mapping[i] not in tensor2rank_variables[tensor]\n",
    "#             if cur_irrelevant:\n",
    "#                 break\n",
    "\n",
    "#         prev_loop = i - 1\n",
    "#         while prev_loop > 0 and not isinstance(mapping[prev_loop], RankVariable):\n",
    "#             prev_loop -= 1\n",
    "\n",
    "#         prev_relevant = True\n",
    "#         if prev_loop >= 0:\n",
    "#             prev_relevant = mapping[prev_loop] in tensor2rank_variables[tensor]\n",
    "#         next_irrelevant = mapping[i+1] not in tensor2rank_variables[tensor]\n",
    "\n",
    "#         if prev_relevant and next_irrelevant:\n",
    "#             mapping2 = copy.copy(mapping)\n",
    "#             mapping2.insert(i, tensor)\n",
    "#             yield mapping2\n",
    "\n",
    "# def place_tensors(node: Storage, tensors: set[Tensor], mapping: list[str]):\n",
    "#     tensors = list(tensors)\n",
    "#     if not tensors:\n",
    "#         yield mapping\n",
    "#         return\n",
    "    \n",
    "#     for mapping2 in place_tensor(node, tensors[0], mapping):\n",
    "#         yield from place_tensors(node, tensors[1:], mapping2)\n",
    "        \n",
    "# def make_temporal_choices_and_place_tensors(node: Storage, tensors: set[Tensor], symbol_table: dict[str, InvertibleSet], mapping: list[str]=None):\n",
    "#     mapping = [] if mapping is None else mapping\n",
    "#     for mapping2 in place_tensors(node, tensors, mapping):\n",
    "#         for loop_order in make_temporal_loop_choices(node, symbol_table):\n",
    "#             yield mapping2 + list(loop_order) \n",
    "            \n",
    "            \n",
    "# # seen = set()\n",
    "# def check_equivalency(mapping: list[str], remaining_nodes: int, tensors_left_to_place: set[Tensor]) -> bool:\n",
    "#     if remaining_nodes == 0:\n",
    "#         check_up_to = len(mapping)\n",
    "#     elif not all(t in mapping for t in tensors_left_to_place):\n",
    "#         return mapping\n",
    "#     else:\n",
    "#         mapping.reverse()\n",
    "#         check_up_to = max(len(mapping) - mapping.index(t) - 1 for t in tensors_left_to_place)\n",
    "#         mapping.reverse()\n",
    "    \n",
    "#     # Sort loop order between any pair of tensors\n",
    "#     for i in range(check_up_to):\n",
    "#         if not isinstance(mapping[i], RankVariable):\n",
    "#             continue\n",
    "#         if i > 0 and isinstance(mapping[i-1], RankVariable):\n",
    "#             continue\n",
    "#         if i < len(mapping) - 1 and not isinstance(mapping[i+1], RankVariable):\n",
    "#             continue\n",
    "        \n",
    "#         j = i + 1\n",
    "#         while j < len(mapping) - 1 and isinstance(mapping[j+1], RankVariable):\n",
    "#             j += 1\n",
    "        \n",
    "#         mapping[i:j+1] = sorted(mapping[i:j+1])\n",
    "        \n",
    "#     # Drop duplicate rank variables\n",
    "#     for i in range(check_up_to - 1, 0, -1):\n",
    "#         if not isinstance(mapping[i], RankVariable):\n",
    "#             continue\n",
    "#         if mapping[i] == mapping[i-1]:\n",
    "#             mapping.pop(i)\n",
    "            \n",
    "#     key = (remaining_nodes, tuple(mapping))\n",
    "#     if key in seen:\n",
    "#         return None\n",
    "#     seen.add(key)\n",
    "#     return mapping\n",
    "            \n",
    "            \n",
    "# def make_temporal_choices_and_place_tensors_all_levels(nodes: list[Storage], storage_choices: dict[str, set[Tensor]], symbol_table: dict[str, InvertibleSet], mapping: list[str]=None):\n",
    "#     while nodes and not isinstance(nodes[0], Storage):\n",
    "#         nodes = nodes[1:]\n",
    "\n",
    "#     mapping = [] if mapping is None else mapping\n",
    "    \n",
    "#     tensors_left_to_place = set().union(*(storage_choices.get(n.name, set()) for n in nodes))\n",
    "\n",
    "#     if len(nodes) == 0:\n",
    "#         mapping = check_equivalency(mapping, len(nodes), tensors_left_to_place)\n",
    "#         if mapping is not None:\n",
    "#             yield mapping\n",
    "#         return\n",
    "    \n",
    "#     if len(mapping) > 30:\n",
    "#         print(\"AHH\")\n",
    "    \n",
    "#     tensors = storage_choices[nodes[0].name]\n",
    "    \n",
    "#     for mapping2 in make_temporal_choices_and_place_tensors(nodes[0], tensors, symbol_table, mapping):\n",
    "#         mapping2 = check_equivalency(mapping2, len(nodes), tensors_left_to_place)\n",
    "#         if mapping2 is None:\n",
    "#             continue\n",
    "#         for mapping3 in make_temporal_choices_and_place_tensors_all_levels(nodes[1:], storage_choices, symbol_table, mapping2):\n",
    "#             yield mapping3\n",
    "\n",
    "# # def place_storage(node: Storage, storage_choices: set[str], mapping: list[str]):\n",
    "# #     mapping = copy.deepcopy(mapping)\n",
    "# #     # storage_options = {}\n",
    "\n",
    "# #     # for tensor in storage_choices.get(node.name, []):\n",
    "# #     #     last_mapping_appearance = max((i for i, t in enumerate(mapping) if t == tensor), default=0)\n",
    "# #     #     for j in range(last_mapping_appearance, len(mapping)):\n",
    "# #     #         prev_loop = mapping[j-1] if j > 0 else None\n",
    "# #     #         prev_relevant = j == 0 or mapping[j-1] in tensor2rank_variables[tensor]\n",
    "            \n",
    "# # TODO: Move parsing to after generating all storage choices\n",
    "        \n",
    "# arch_nodes = [n for n in arch_nodes if isinstance(n, Storage)]\n",
    "    \n",
    "# i = 0\n",
    "# for storage_choices, symbol_table2 in make_storage_choices_all_levels(arch_nodes, symbol_table):\n",
    "#     seen = set()\n",
    "#     print(f'\\n\\nStorage choices: {storage_choices}')\n",
    "#     for mapping in make_temporal_choices_and_place_tensors_all_levels(arch_nodes, storage_choices, symbol_table2):\n",
    "#         # print(f'\\t{mapping}')\n",
    "#         i += 1\n",
    "#         # if i % 1000 == 0:\n",
    "#         print(f'\\t{i}: {mapping}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import itertools\n",
    "# from typing import Generator\n",
    "# from fastfusion.frontend._set_parse import InvertibleSet\n",
    "# from fastfusion.frontend.arch import Leaf, Storage\n",
    "# from fastfusion.frontend.workload.workload_spec import RankVariable, Tensor\n",
    "\n",
    "\n",
    "# einsum_name = \"Q\"\n",
    "# einsum = workload.einsums[einsum_name]\n",
    "# rank_variables = einsum.rank_variables\n",
    "# tensors = einsum.tensors\n",
    "# symbol_table = workload.get_constraint_symbol_table(einsum_name, renames)\n",
    "# first_value = next(iter(symbol_table.values()))\n",
    "# arch_nodes = spec.get_flattened_architecture()\n",
    "# tensor2rank_variables = einsum.tensor2rank_variables\n",
    "\n",
    "# from itertools import chain, combinations\n",
    "# def powerset(iterable):\n",
    "#     \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "#     s = list(iterable)\n",
    "#     return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "# def make_storage_choices_one_level(node: Leaf, symbol_table: dict[str, InvertibleSet]):\n",
    "#     if not isinstance(node, Storage):\n",
    "#         yield set(), symbol_table\n",
    "#         return\n",
    "#     new_symbol_table = copy.copy(symbol_table)\n",
    "#     storage_constraints = node.constraints.storage._parse(symbol_table)\n",
    "#     must_keep = first_value.to_my_space(storage_constraints[\"keep\"])\n",
    "#     must_bypass = first_value.to_my_space(storage_constraints[\"bypass\"])\n",
    "    \n",
    "#     if must_keep - new_symbol_table[\"All\"]:\n",
    "#         raise KeyError(f\"Keep constraint for {node.name} includes tensors that are not in the einsum: {must_keep - new_symbol_table['All']}\")\n",
    "#     if must_bypass - new_symbol_table[\"All\"]:\n",
    "#         raise KeyError(f\"Bypass constraint for {node.name} includes tensors that are not in the einsum: {must_bypass - tensors}\")\n",
    "#     if must_keep & must_bypass:\n",
    "#         raise KeyError(f\"Keep and bypass constraints for {node.name} intersect: {must_keep & must_bypass}\")\n",
    "    \n",
    "    \n",
    "#     may_keep = tensors - must_bypass - must_keep\n",
    "#     for subset in powerset(may_keep):\n",
    "#         subset = first_value.to_my_space(set(subset))\n",
    "#         keep_choice = first_value.to_my_space(subset | must_keep)\n",
    "#         keep_choice.tensors = lambda: keep_choice # So users can do MainMemory().tensors(). Optional.\n",
    "#         new_symbol_table[node.name] = keep_choice\n",
    "#         assert not any(isinstance(k, str) for k in keep_choice)\n",
    "#         yield keep_choice, new_symbol_table\n",
    "\n",
    "# def make_temporal_loop_choices(node: Storage, symbol_table: dict[str, InvertibleSet]) -> Generator[list[str], None, None]:\n",
    "#     if not isinstance(node, Storage):\n",
    "#         yield set()\n",
    "#         return\n",
    "    \n",
    "#     temporal_constraints = node.constraints.temporal._parse(symbol_table)\n",
    "#     for i in range(len(temporal_constraints.loop_order)):\n",
    "#         for j in range(i + 1, len(temporal_constraints.loop_order)):\n",
    "#             if temporal_constraints.loop_order[i] & temporal_constraints.loop_order[j]:\n",
    "#                 raise ValueError(\n",
    "#                     f\"Loop order constraint indices {i} and {j} for {node.name} intersect:\"\n",
    "#                     f\" {temporal_constraints.loop_order[i] & temporal_constraints.loop_order[j]}\"\n",
    "#                 )\n",
    "\n",
    "#     loop_order_constraint = temporal_constraints.loop_order\n",
    "#     loop_order_ranks = set.union(*(set(s) for s in loop_order_constraint), set())\n",
    "#     def is_valid_loop_order(loop_order):\n",
    "#         loop_order = [l for l in loop_order if l in loop_order_ranks]\n",
    "#         if not loop_order:\n",
    "#             return True\n",
    "\n",
    "#         i = 0\n",
    "#         for rank_variable in loop_order:\n",
    "#             if i >= len(loop_order_constraint):\n",
    "#                 return False\n",
    "#             while rank_variable not in loop_order_constraint[i]:\n",
    "#                 i += 1\n",
    "#                 if i >= len(loop_order_constraint):\n",
    "#                     return False\n",
    "#         return True\n",
    "    \n",
    "#     to_permute = rank_variables - temporal_constraints.loop_bounds.get_constrained_one_rank_variables()\n",
    "#     for loop_order in itertools.permutations(to_permute, len(to_permute)):\n",
    "#         if is_valid_loop_order(loop_order):\n",
    "#             yield loop_order\n",
    "            \n",
    "# def make_storage_choices_all_levels(nodes: list[Storage], symbol_table: dict[str, InvertibleSet]):\n",
    "#     while nodes and not isinstance(nodes[0], Storage):\n",
    "#         nodes = nodes[1:]\n",
    "#     if len(nodes) == 0:\n",
    "#         yield dict(), symbol_table\n",
    "#         return\n",
    "\n",
    "#     for choice, symbol_table in make_storage_choices_one_level(nodes[0], symbol_table):\n",
    "#         for subchoices, symbol_table in make_storage_choices_all_levels(nodes[1:], symbol_table):\n",
    "#             yield {**subchoices, nodes[0].name: choice}, symbol_table\n",
    "\n",
    "    \n",
    "# # UNEVEN = True        \n",
    "# # def uneven_storage(node: Leaf, symbol_table: dict[str, InvertibleSet], storage_choice: set[str], loop_order: list[str], previous_loop=None):\n",
    "# #     if not UNEVEN:\n",
    "# #         return {t: 0 for t in storage_choice}\n",
    "    \n",
    "# #     if previous_loop is None:\n",
    "# #         previous_loop = set()\n",
    "# #     for i, t in enumerate(loop_order):\n",
    "# #         if t in previous_loop:\n",
    "# #             continue\n",
    "# #         previous_loop.add(t)\n",
    "# #         yield from uneven_storage(node, symbol_table, storage_choice, loop_order, previous_loop)\n",
    "\n",
    "# def place_tensor(node: Storage, tensor: Tensor, mapping: list[str]) -> Generator[list[str], None, None]:\n",
    "#     if len([x for x in mapping if isinstance(x, RankVariable)]) <= 1:\n",
    "#         yield [tensor] + mapping\n",
    "#         return\n",
    "    \n",
    "#     last_mapping_appearance = max((i for i, t in enumerate(mapping) if t == tensor), default=None)\n",
    "#     appeared_before = True\n",
    "#     if last_mapping_appearance is None:\n",
    "#         appeared_before = False\n",
    "#         last_mapping_appearance = 0\n",
    "\n",
    "#     for i in range(last_mapping_appearance, len(mapping) - 1):\n",
    "#         # We're not worried about relative ordering of storage. Only storages vs. loops.\n",
    "#         if not isinstance(mapping[i], RankVariable):\n",
    "#             continue\n",
    "\n",
    "#         if not appeared_before:\n",
    "#             cur_irrelevant = mapping[i] not in tensor2rank_variables[tensor]\n",
    "#             if cur_irrelevant:\n",
    "#                 break\n",
    "\n",
    "#         prev_loop = i - 1\n",
    "#         while prev_loop > 0 and not isinstance(mapping[prev_loop], RankVariable):\n",
    "#             prev_loop -= 1\n",
    "\n",
    "#         prev_relevant = True\n",
    "#         if prev_loop >= 0:\n",
    "#             prev_relevant = mapping[prev_loop] in tensor2rank_variables[tensor]\n",
    "#         next_irrelevant = mapping[i+1] not in tensor2rank_variables[tensor]\n",
    "\n",
    "#         if prev_relevant and next_irrelevant:\n",
    "#             mapping2 = copy.copy(mapping)\n",
    "#             mapping2.insert(i, tensor)\n",
    "#             yield mapping2\n",
    "\n",
    "# def place_tensors(node: Storage, tensors: set[Tensor], mapping: list[str]):\n",
    "#     tensors = list(tensors)\n",
    "#     if not tensors:\n",
    "#         yield mapping\n",
    "#         return\n",
    "    \n",
    "#     for mapping2 in place_tensor(node, tensors[0], mapping):\n",
    "#         yield from place_tensors(node, tensors[1:], mapping2)\n",
    "        \n",
    "# def make_temporal_choices_and_place_tensors(node: Storage, tensors: set[Tensor], symbol_table: dict[str, InvertibleSet], mapping: list[str]=None):\n",
    "#     mapping = [] if mapping is None else mapping\n",
    "#     for mapping2 in place_tensors(node, tensors, mapping):\n",
    "#         for loop_order in make_temporal_loop_choices(node, symbol_table):\n",
    "#             yield mapping2 + list(loop_order) \n",
    "            \n",
    "            \n",
    "# # seen = set()\n",
    "# def check_equivalency(mapping: list[str], remaining_nodes: int, tensors_left_to_place: set[Tensor]) -> bool:\n",
    "#     if remaining_nodes == 0:\n",
    "#         check_up_to = len(mapping)\n",
    "#     elif not all(t in mapping for t in tensors_left_to_place):\n",
    "#         return mapping\n",
    "#     else:\n",
    "#         mapping.reverse()\n",
    "#         check_up_to = max(len(mapping) - mapping.index(t) - 1 for t in tensors_left_to_place)\n",
    "#         mapping.reverse()\n",
    "    \n",
    "#     # Sort loop order between any pair of tensors\n",
    "#     for i in range(check_up_to):\n",
    "#         if not isinstance(mapping[i], RankVariable):\n",
    "#             continue\n",
    "#         if i > 0 and isinstance(mapping[i-1], RankVariable):\n",
    "#             continue\n",
    "#         if i < len(mapping) - 1 and not isinstance(mapping[i+1], RankVariable):\n",
    "#             continue\n",
    "        \n",
    "#         j = i + 1\n",
    "#         while j < len(mapping) - 1 and isinstance(mapping[j+1], RankVariable):\n",
    "#             j += 1\n",
    "        \n",
    "#         mapping[i:j+1] = sorted(mapping[i:j+1])\n",
    "        \n",
    "#     # Drop duplicate rank variables\n",
    "#     for i in range(check_up_to - 1, 0, -1):\n",
    "#         if not isinstance(mapping[i], RankVariable):\n",
    "#             continue\n",
    "#         if mapping[i] == mapping[i-1]:\n",
    "#             mapping.pop(i)\n",
    "            \n",
    "#     key = (remaining_nodes, tuple(mapping))\n",
    "#     if key in seen:\n",
    "#         return None\n",
    "#     seen.add(key)\n",
    "#     return mapping\n",
    "            \n",
    "            \n",
    "# def make_temporal_choices_and_place_tensors_all_levels(nodes: list[Storage], storage_choices: dict[str, set[Tensor]], symbol_table: dict[str, InvertibleSet], mapping: list[str]=None):\n",
    "#     while nodes and not isinstance(nodes[0], Storage):\n",
    "#         nodes = nodes[1:]\n",
    "\n",
    "#     mapping = [] if mapping is None else mapping\n",
    "    \n",
    "#     tensors_left_to_place = set().union(*(storage_choices.get(n.name, set()) for n in nodes))\n",
    "\n",
    "#     if len(nodes) == 0:\n",
    "#         mapping = check_equivalency(mapping, len(nodes), tensors_left_to_place)\n",
    "#         if mapping is not None:\n",
    "#             yield mapping\n",
    "#         return\n",
    "    \n",
    "#     if len(mapping) > 30:\n",
    "#         print(\"AHH\")\n",
    "    \n",
    "#     tensors = storage_choices[nodes[0].name]\n",
    "    \n",
    "#     for mapping2 in make_temporal_choices_and_place_tensors(nodes[0], tensors, symbol_table, mapping):\n",
    "#         mapping2 = check_equivalency(mapping2, len(nodes), tensors_left_to_place)\n",
    "#         if mapping2 is None:\n",
    "#             continue\n",
    "#         for mapping3 in make_temporal_choices_and_place_tensors_all_levels(nodes[1:], storage_choices, symbol_table, mapping2):\n",
    "#             yield mapping3\n",
    "\n",
    "# # def place_storage(node: Storage, storage_choices: set[str], mapping: list[str]):\n",
    "# #     mapping = copy.deepcopy(mapping)\n",
    "# #     # storage_options = {}\n",
    "\n",
    "# #     # for tensor in storage_choices.get(node.name, []):\n",
    "# #     #     last_mapping_appearance = max((i for i, t in enumerate(mapping) if t == tensor), default=0)\n",
    "# #     #     for j in range(last_mapping_appearance, len(mapping)):\n",
    "# #     #         prev_loop = mapping[j-1] if j > 0 else None\n",
    "# #     #         prev_relevant = j == 0 or mapping[j-1] in tensor2rank_variables[tensor]\n",
    "            \n",
    "# # TODO: Move parsing to after generating all storage choices\n",
    "        \n",
    "# # arch_nodes = [n for n in arch_nodes if isinstance(n, Storage)]\n",
    "    \n",
    "# # i = 0\n",
    "# # for storage_choices, symbol_table2 in make_storage_choices_all_levels(arch_nodes, symbol_table):\n",
    "# #     seen = set()\n",
    "# #     print(f'\\n\\nStorage choices: {storage_choices}')\n",
    "# #     for mapping in make_temporal_choices_and_place_tensors_all_levels(arch_nodes, storage_choices, symbol_table2):\n",
    "# #         # print(f'\\t{mapping}')\n",
    "# #         i += 1\n",
    "# #         # if i % 1000 == 0:\n",
    "# #         print(f'\\t{i}: {mapping}')\n",
    "        \n",
    "# def get_storage_loop_blocks(i: int, mapping: list[str]) -> tuple[list[Tensor], list[RankVariable], list[Tensor]]:\n",
    "#     if not isinstance(mapping[i], RankVariable):\n",
    "#         raise ValueError(f\"Index {i} must point to a RankVariable\")\n",
    "        \n",
    "#     left_tensors = []\n",
    "#     right_tensors = []\n",
    "    \n",
    "#     # Find the start of the rank variable block\n",
    "#     rank_start = i\n",
    "#     while rank_start > 0 and isinstance(mapping[rank_start - 1], RankVariable):\n",
    "#         rank_start -= 1\n",
    "    \n",
    "#     # Find the end of the rank variable block\n",
    "#     rank_end = i\n",
    "#     while rank_end < len(mapping) - 1 and isinstance(mapping[rank_end + 1], RankVariable):\n",
    "#         rank_end += 1\n",
    "    \n",
    "#     # Find the start of the left tensor block\n",
    "#     left_tensor_start = rank_start\n",
    "#     while left_tensor_start > 0 and isinstance(mapping[left_tensor_start - 1], Tensor):\n",
    "#         left_tensor_start -= 1\n",
    "    \n",
    "#     # Find the end of the right tensor block\n",
    "#     right_tensor_end = rank_end\n",
    "#     while right_tensor_end < len(mapping) - 1 and isinstance(mapping[right_tensor_end + 1], Tensor):\n",
    "#         right_tensor_end += 1\n",
    "    \n",
    "#     # Extract the blocks\n",
    "#     left_tensors = mapping[left_tensor_start:rank_start]\n",
    "#     ranks = mapping[rank_start:rank_end + 1]\n",
    "#     right_tensors = mapping[rank_end + 1:right_tensor_end + 1]\n",
    "    \n",
    "#     return left_tensors, ranks, right_tensors        \n",
    "        \n",
    "# def is_valid(mapping):\n",
    "#     # Back-to-back rank variables must be ordered\n",
    "#     for i in range(len(mapping) - 1):\n",
    "#         if isinstance(mapping[i], RankVariable) and isinstance(mapping[i+1], RankVariable):\n",
    "#             if mapping[i] >= mapping[i+1]:\n",
    "#                 return False\n",
    "\n",
    "#     # Back to back storages must be ordered\n",
    "#     for i in range(len(mapping) - 1):\n",
    "#         if isinstance(mapping[i], Storage) and isinstance(mapping[i+1], Storage):\n",
    "#             if mapping[i] > mapping[i+1]:\n",
    "#                 return False\n",
    "\n",
    "#     # Above the first appearance of a tensor, all rank variables must be irrelevant\n",
    "#     for t in tensors:\n",
    "#         for i in range(len(mapping)):\n",
    "#             if mapping[i] == t:\n",
    "#                 break\n",
    "#             if isinstance(mapping[i], RankVariable):\n",
    "#                 if mapping[i] not in tensor2rank_variables[t]:\n",
    "#                     return False\n",
    "    \n",
    "#     # Looking at any contiguous block of rank variables:\n",
    "#     # - Grab the left contiguous block of tensors.\n",
    "#     # - Grab the right contiguous block of tensors.\n",
    "#     # - If the rank variable is relevant to all left tensors, it's invalid.\n",
    "#     # - If the rank variable is irrelevant to all right tensors, it's invalid.\n",
    "#     for i in range(len(mapping)):\n",
    "#         if isinstance(mapping[i], RankVariable):\n",
    "#             left_tensors, ranks, right_tensors = get_storage_loop_blocks(i, mapping)\n",
    "#             if left_tensors:\n",
    "#                 if any(all(r in tensor2rank_variables[t] for t in left_tensors) for r in ranks):\n",
    "#                     return False\n",
    "#             if right_tensors:\n",
    "#                 if any(all(r not in tensor2rank_variables[t] for t in right_tensors) for r in ranks):\n",
    "#                     return False\n",
    "#     return True\n",
    "    \n",
    "# def recursive_build_mapping(\n",
    "#     mapping: list[str],\n",
    "#     nodes: list[Storage],\n",
    "#     tensor2remaining: dict[Tensor, int],\n",
    "# ):\n",
    "#     if not tensor2remaining or not any(tensor2remaining.values()):\n",
    "#         yield mapping\n",
    "#         return\n",
    "    \n",
    "#     choices = [t for t in tensor2remaining if tensor2remaining[t] > 0]\n",
    "#     choices += rank_variables\n",
    "    \n",
    "#     for choice in choices:\n",
    "#         mapping2 = mapping + [choice]\n",
    "#         # Check for violation\n",
    "#         if is_valid(mapping2):\n",
    "#             if choice in tensors:\n",
    "#                 tensor2remaining[choice] -= 1\n",
    "#             yield from recursive_build_mapping(mapping2, nodes, tensor2remaining)\n",
    "#             if choice in tensors:\n",
    "#                 tensor2remaining[choice] += 1\n",
    "    \n",
    "    \n",
    "# # If there are two back-to-back storages for the same tensor & the outer is\n",
    "# # optional, then it is invalid.\n",
    "\n",
    "# storage_choice_options = [s for s, _ in make_storage_choices_all_levels(arch_nodes, symbol_table)]\n",
    "\n",
    "# for storage_choices in storage_choice_options[8:]:\n",
    "#     print(f'{storage_choices}')\n",
    "#     main_memory = arch_nodes[0]\n",
    "#     tensor2remaining = {t: 0 for t in tensors}\n",
    "#     for k, v in storage_choices.items():\n",
    "#         if k != main_memory.name:\n",
    "#             for t in v:\n",
    "#                 tensor2remaining[t] += 1\n",
    "#     mapping_base = list(storage_choices[main_memory.name])\n",
    "#     for mapping in recursive_build_mapping(mapping_base, arch_nodes[1:], tensor2remaining):\n",
    "#         print(mapping)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastfusion.frontend._set_parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generator\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastfusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_set_parse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvertibleSet\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastfusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchitecture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Leaf, Storage\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastfusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RankVariable, Tensor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastfusion.frontend._set_parse'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "from typing import Generator\n",
    "from fastfusion.frontend._set_parse import InvertibleSet\n",
    "from fastfusion.frontend.architecture import Leaf, Storage\n",
    "from fastfusion.frontend.workload.workload import RankVariable, Tensor\n",
    "\n",
    "\n",
    "einsum_name = \"Q\"\n",
    "einsum = workload.einsums[einsum_name]\n",
    "rank_variables = einsum.rank_variables\n",
    "tensors = einsum.tensors\n",
    "symbol_table = workload.get_constraint_symbol_table(einsum_name, renames)\n",
    "first_value = next(iter(symbol_table.values()))\n",
    "arch_nodes = spec.get_flattened_architecture()\n",
    "tensor2rank_variables = einsum.tensor2rank_variables\n",
    "\n",
    "from itertools import chain, combinations\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "def make_storage_choices_one_level(node: Leaf, symbol_table: dict[str, InvertibleSet]):\n",
    "    if not isinstance(node, Storage):\n",
    "        yield set(), symbol_table\n",
    "        return\n",
    "    new_symbol_table = copy.copy(symbol_table)\n",
    "    storage_constraints = node.constraints.storage._parse(symbol_table)\n",
    "    must_keep = first_value.to_my_space(storage_constraints[\"keep\"])\n",
    "    must_bypass = first_value.to_my_space(storage_constraints[\"bypass\"])\n",
    "    \n",
    "    if must_keep - new_symbol_table[\"All\"]:\n",
    "        raise KeyError(f\"Keep constraint for {node.name} includes tensors that are not in the einsum: {must_keep - new_symbol_table['All']}\")\n",
    "    if must_bypass - new_symbol_table[\"All\"]:\n",
    "        raise KeyError(f\"Bypass constraint for {node.name} includes tensors that are not in the einsum: {must_bypass - tensors}\")\n",
    "    if must_keep & must_bypass:\n",
    "        raise KeyError(f\"Keep and bypass constraints for {node.name} intersect: {must_keep & must_bypass}\")\n",
    "    \n",
    "    \n",
    "    may_keep = tensors - must_bypass - must_keep\n",
    "    for subset in powerset(may_keep):\n",
    "        subset = first_value.to_my_space(set(subset))\n",
    "        keep_choice = first_value.to_my_space(subset | must_keep)\n",
    "        keep_choice.tensors = lambda: keep_choice # So users can do MainMemory().tensors(). Optional.\n",
    "        new_symbol_table[node.name] = keep_choice\n",
    "        assert not any(isinstance(k, str) for k in keep_choice)\n",
    "        yield keep_choice, new_symbol_table\n",
    "\n",
    "def make_temporal_loop_choices(node: Storage, symbol_table: dict[str, InvertibleSet]) -> Generator[list[str], None, None]:\n",
    "    if not isinstance(node, Storage):\n",
    "        yield set()\n",
    "        return\n",
    "    \n",
    "    temporal_constraints = node.constraints.temporal._parse(symbol_table)\n",
    "    for i in range(len(temporal_constraints.loop_order)):\n",
    "        for j in range(i + 1, len(temporal_constraints.loop_order)):\n",
    "            if temporal_constraints.loop_order[i] & temporal_constraints.loop_order[j]:\n",
    "                raise ValueError(\n",
    "                    f\"Loop order constraint indices {i} and {j} for {node.name} intersect:\"\n",
    "                    f\" {temporal_constraints.loop_order[i] & temporal_constraints.loop_order[j]}\"\n",
    "                )\n",
    "\n",
    "    loop_order_constraint = temporal_constraints.loop_order\n",
    "    loop_order_ranks = set.union(*(set(s) for s in loop_order_constraint), set())\n",
    "    def is_valid_loop_order(loop_order):\n",
    "        loop_order = [l for l in loop_order if l in loop_order_ranks]\n",
    "        if not loop_order:\n",
    "            return True\n",
    "\n",
    "        i = 0\n",
    "        for rank_variable in loop_order:\n",
    "            if i >= len(loop_order_constraint):\n",
    "                return False\n",
    "            while rank_variable not in loop_order_constraint[i]:\n",
    "                i += 1\n",
    "                if i >= len(loop_order_constraint):\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    to_permute = rank_variables - temporal_constraints.loop_bounds.get_constrained_one_rank_variables()\n",
    "    for loop_order in itertools.permutations(to_permute, len(to_permute)):\n",
    "        if is_valid_loop_order(loop_order):\n",
    "            yield loop_order\n",
    "            \n",
    "def make_storage_choices_all_levels(nodes: list[Storage], symbol_table: dict[str, InvertibleSet]):\n",
    "    while nodes and not isinstance(nodes[0], Storage):\n",
    "        nodes = nodes[1:]\n",
    "    if len(nodes) == 0:\n",
    "        yield dict(), symbol_table\n",
    "        return\n",
    "\n",
    "    for choice, symbol_table in make_storage_choices_one_level(nodes[0], symbol_table):\n",
    "        for subchoices, symbol_table in make_storage_choices_all_levels(nodes[1:], symbol_table):\n",
    "            yield {**subchoices, nodes[0].name: choice}, symbol_table\n",
    "\n",
    "    \n",
    "# UNEVEN = True        \n",
    "# def uneven_storage(node: Leaf, symbol_table: dict[str, InvertibleSet], storage_choice: set[str], loop_order: list[str], previous_loop=None):\n",
    "#     if not UNEVEN:\n",
    "#         return {t: 0 for t in storage_choice}\n",
    "    \n",
    "#     if previous_loop is None:\n",
    "#         previous_loop = set()\n",
    "#     for i, t in enumerate(loop_order):\n",
    "#         if t in previous_loop:\n",
    "#             continue\n",
    "#         previous_loop.add(t)\n",
    "#         yield from uneven_storage(node, symbol_table, storage_choice, loop_order, previous_loop)\n",
    "\n",
    "def place_tensor(node: Storage, tensor: Tensor, mapping: list[str]) -> Generator[list[str], None, None]:\n",
    "    if len([x for x in mapping if isinstance(x, RankVariable)]) <= 1:\n",
    "        yield [tensor] + mapping\n",
    "        return\n",
    "    \n",
    "    last_mapping_appearance = max((i for i, t in enumerate(mapping) if t == tensor), default=None)\n",
    "    appeared_before = True\n",
    "    if last_mapping_appearance is None:\n",
    "        appeared_before = False\n",
    "        last_mapping_appearance = 0\n",
    "\n",
    "    for i in range(last_mapping_appearance, len(mapping) - 1):\n",
    "        # We're not worried about relative ordering of storage. Only storages vs. loops.\n",
    "        if not isinstance(mapping[i], RankVariable):\n",
    "            continue\n",
    "\n",
    "        if not appeared_before:\n",
    "            cur_irrelevant = mapping[i] not in tensor2rank_variables[tensor]\n",
    "            if cur_irrelevant:\n",
    "                break\n",
    "\n",
    "        prev_loop = i - 1\n",
    "        while prev_loop > 0 and not isinstance(mapping[prev_loop], RankVariable):\n",
    "            prev_loop -= 1\n",
    "\n",
    "        prev_relevant = True\n",
    "        if prev_loop >= 0:\n",
    "            prev_relevant = mapping[prev_loop] in tensor2rank_variables[tensor]\n",
    "        next_irrelevant = mapping[i+1] not in tensor2rank_variables[tensor]\n",
    "\n",
    "        if prev_relevant and next_irrelevant:\n",
    "            mapping2 = copy.copy(mapping)\n",
    "            mapping2.insert(i, tensor)\n",
    "            yield mapping2\n",
    "\n",
    "def place_tensors(node: Storage, tensors: set[Tensor], mapping: list[str]):\n",
    "    tensors = list(tensors)\n",
    "    if not tensors:\n",
    "        yield mapping\n",
    "        return\n",
    "    \n",
    "    for mapping2 in place_tensor(node, tensors[0], mapping):\n",
    "        yield from place_tensors(node, tensors[1:], mapping2)\n",
    "        \n",
    "def make_temporal_choices_and_place_tensors(node: Storage, tensors: set[Tensor], symbol_table: dict[str, InvertibleSet], mapping: list[str]=None):\n",
    "    mapping = [] if mapping is None else mapping\n",
    "    for mapping2 in place_tensors(node, tensors, mapping):\n",
    "        for loop_order in make_temporal_loop_choices(node, symbol_table):\n",
    "            yield mapping2 + list(loop_order) \n",
    "            \n",
    "            \n",
    "# seen = set()\n",
    "def check_equivalency(mapping: list[str], remaining_nodes: int, tensors_left_to_place: set[Tensor]) -> bool:\n",
    "    if remaining_nodes == 0:\n",
    "        check_up_to = len(mapping)\n",
    "    elif not all(t in mapping for t in tensors_left_to_place):\n",
    "        return mapping\n",
    "    else:\n",
    "        mapping.reverse()\n",
    "        check_up_to = max(len(mapping) - mapping.index(t) - 1 for t in tensors_left_to_place)\n",
    "        mapping.reverse()\n",
    "    \n",
    "    # Sort loop order between any pair of tensors\n",
    "    for i in range(check_up_to):\n",
    "        if not isinstance(mapping[i], RankVariable):\n",
    "            continue\n",
    "        if i > 0 and isinstance(mapping[i-1], RankVariable):\n",
    "            continue\n",
    "        if i < len(mapping) - 1 and not isinstance(mapping[i+1], RankVariable):\n",
    "            continue\n",
    "        \n",
    "        j = i + 1\n",
    "        while j < len(mapping) - 1 and isinstance(mapping[j+1], RankVariable):\n",
    "            j += 1\n",
    "        \n",
    "        mapping[i:j+1] = sorted(mapping[i:j+1])\n",
    "        \n",
    "    # Drop duplicate rank variables\n",
    "    for i in range(check_up_to - 1, 0, -1):\n",
    "        if not isinstance(mapping[i], RankVariable):\n",
    "            continue\n",
    "        if mapping[i] == mapping[i-1]:\n",
    "            mapping.pop(i)\n",
    "            \n",
    "    key = (remaining_nodes, tuple(mapping))\n",
    "    if key in seen:\n",
    "        return None\n",
    "    seen.add(key)\n",
    "    return mapping\n",
    "            \n",
    "            \n",
    "def make_temporal_choices_and_place_tensors_all_levels(nodes: list[Storage], storage_choices: dict[str, set[Tensor]], symbol_table: dict[str, InvertibleSet], mapping: list[str]=None):\n",
    "    while nodes and not isinstance(nodes[0], Storage):\n",
    "        nodes = nodes[1:]\n",
    "\n",
    "    mapping = [] if mapping is None else mapping\n",
    "    \n",
    "    tensors_left_to_place = set().union(*(storage_choices.get(n.name, set()) for n in nodes))\n",
    "\n",
    "    if len(nodes) == 0:\n",
    "        mapping = check_equivalency(mapping, len(nodes), tensors_left_to_place)\n",
    "        if mapping is not None:\n",
    "            yield mapping\n",
    "        return\n",
    "    \n",
    "    if len(mapping) > 30:\n",
    "        print(\"AHH\")\n",
    "    \n",
    "    tensors = storage_choices[nodes[0].name]\n",
    "    \n",
    "    for mapping2 in make_temporal_choices_and_place_tensors(nodes[0], tensors, symbol_table, mapping):\n",
    "        mapping2 = check_equivalency(mapping2, len(nodes), tensors_left_to_place)\n",
    "        if mapping2 is None:\n",
    "            continue\n",
    "        for mapping3 in make_temporal_choices_and_place_tensors_all_levels(nodes[1:], storage_choices, symbol_table, mapping2):\n",
    "            yield mapping3\n",
    "\n",
    "# def place_storage(node: Storage, storage_choices: set[str], mapping: list[str]):\n",
    "#     mapping = copy.deepcopy(mapping)\n",
    "#     # storage_options = {}\n",
    "\n",
    "#     # for tensor in storage_choices.get(node.name, []):\n",
    "#     #     last_mapping_appearance = max((i for i, t in enumerate(mapping) if t == tensor), default=0)\n",
    "#     #     for j in range(last_mapping_appearance, len(mapping)):\n",
    "#     #         prev_loop = mapping[j-1] if j > 0 else None\n",
    "#     #         prev_relevant = j == 0 or mapping[j-1] in tensor2rank_variables[tensor]\n",
    "            \n",
    "# TODO: Move parsing to after generating all storage choices\n",
    "        \n",
    "# arch_nodes = [n for n in arch_nodes if isinstance(n, Storage)]\n",
    "    \n",
    "# i = 0\n",
    "# for storage_choices, symbol_table2 in make_storage_choices_all_levels(arch_nodes, symbol_table):\n",
    "#     seen = set()\n",
    "#     print(f'\\n\\nStorage choices: {storage_choices}')\n",
    "#     for mapping in make_temporal_choices_and_place_tensors_all_levels(arch_nodes, storage_choices, symbol_table2):\n",
    "#         # print(f'\\t{mapping}')\n",
    "#         i += 1\n",
    "#         # if i % 1000 == 0:\n",
    "#         print(f'\\t{i}: {mapping}')\n",
    "        \n",
    "def get_storage_loop_blocks(i: int, mapping: list[str], first_non_fused_loop: int) -> tuple[list[Tensor], list[RankVariable], list[Tensor]]:\n",
    "    if not isinstance(mapping[i], RankVariable):\n",
    "        raise ValueError(f\"Index {i} must point to a RankVariable\")\n",
    "        \n",
    "    left_tensors = []\n",
    "    right_tensors = []\n",
    "    \n",
    "    # Find the start of the rank variable block\n",
    "    rank_start = i\n",
    "    while rank_start > 0 and isinstance(mapping[rank_start - 1], RankVariable):\n",
    "        rank_start -= 1\n",
    "    \n",
    "    # Find the end of the rank variable block\n",
    "    rank_end = i\n",
    "    while rank_end < len(mapping) - 1 and isinstance(mapping[rank_end + 1], RankVariable):\n",
    "        rank_end += 1\n",
    "    \n",
    "    # Find the start of the left tensor block\n",
    "    left_tensor_start = rank_start\n",
    "    while left_tensor_start > 0 and isinstance(mapping[left_tensor_start - 1], Tensor):\n",
    "        left_tensor_start -= 1\n",
    "    \n",
    "    # Find the end of the right tensor block\n",
    "    right_tensor_end = rank_end\n",
    "    while right_tensor_end < len(mapping) - 1 and isinstance(mapping[right_tensor_end + 1], Tensor):\n",
    "        right_tensor_end += 1\n",
    "    \n",
    "    # Extract the blocks\n",
    "    rank_start = max(rank_start, first_non_fused_loop)\n",
    "    left_tensors = mapping[left_tensor_start:rank_start]\n",
    "    ranks = mapping[rank_start:rank_end + 1]\n",
    "    right_tensors = mapping[rank_end + 1:right_tensor_end + 1]\n",
    "    \n",
    "    return left_tensors, ranks, right_tensors        \n",
    "        \n",
    "def is_valid(mapping):\n",
    "    return True\n",
    "    # Above the first appearance of a tensor, all rank variables must be irrelevant\n",
    "    for t in tensors:\n",
    "        for i in range(0, len(mapping)):\n",
    "            if mapping[i] == t:\n",
    "                break\n",
    "            if isinstance(mapping[i], RankVariable):\n",
    "                if mapping[i] not in tensor2rank_variables[t]:\n",
    "                    return False\n",
    "\n",
    "    # No multiple loops over the same rank\n",
    "    for i in range(len(mapping)):\n",
    "        for j in range(i-1, -1, -1):\n",
    "            if not isinstance(mapping[j], RankVariable):\n",
    "                break\n",
    "            if mapping[i] == mapping[j]:\n",
    "                return False\n",
    "\n",
    "    # No further rules until all backing storages have been seen\n",
    "    seen_tensors = set()\n",
    "    first_non_fused_loop = len(mapping)\n",
    "    for i in range(len(mapping)):\n",
    "        if isinstance(mapping[i], Tensor):\n",
    "            seen_tensors.add(mapping[i])\n",
    "        if len(seen_tensors) == len(tensors):\n",
    "            first_non_fused_loop = i\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Back-to-back rank variables must be ordered\n",
    "    for i in range(first_non_fused_loop, len(mapping) - 1):\n",
    "        if isinstance(mapping[i], RankVariable) and isinstance(mapping[i+1], RankVariable):\n",
    "            if mapping[i] >= mapping[i+1]:\n",
    "                return False\n",
    "\n",
    "    # Back to back storages must be ordered\n",
    "    for i in range(first_non_fused_loop, len(mapping) - 1):\n",
    "        if isinstance(mapping[i], Storage) and isinstance(mapping[i+1], Storage):\n",
    "            if mapping[i] > mapping[i+1]:\n",
    "                return False\n",
    "    \n",
    "    # Looking at any contiguous block of rank variables:\n",
    "    # - Grab the left contiguous block of tensors.\n",
    "    # - Grab the right contiguous block of tensors.\n",
    "    # - If the rank variable is relevant to all left tensors, it's invalid.\n",
    "    # - If the rank variable is irrelevant to all right tensors, it's invalid.\n",
    "    for i in range(len(mapping)):\n",
    "        if not isinstance(mapping[i], RankVariable):\n",
    "            continue\n",
    "        left_tensors, ranks, right_tensors = get_storage_loop_blocks(i, mapping, 0)\n",
    "        if left_tensors:\n",
    "            if any(all(r in tensor2rank_variables[t] for t in left_tensors) for r in ranks):\n",
    "                return False\n",
    "        if right_tensors:\n",
    "            if any(all(r not in tensor2rank_variables[t] for t in right_tensors) for r in ranks):\n",
    "                return False\n",
    "\n",
    "    # LRP: Each tensor must be immediately above an irrelevant rank variable and\n",
    "    # immediately below a relevant rank variable. Else it'd be better to move\n",
    "    # the tensor down or up.\n",
    "    for i in range(first_non_fused_loop, len(mapping)):\n",
    "        if not isinstance(mapping[i], Tensor):\n",
    "            continue\n",
    "\n",
    "        j = i - 1\n",
    "        while j >= first_non_fused_loop:\n",
    "            if isinstance(mapping[j], RankVariable):\n",
    "                if mapping[j] not in tensor2rank_variables[mapping[i]]:\n",
    "                    return False\n",
    "                break\n",
    "            j -= 1\n",
    "        j = i + 1\n",
    "        while j < len(mapping):\n",
    "            if isinstance(mapping[j], RankVariable):\n",
    "                if mapping[j] in tensor2rank_variables[mapping[i]]:\n",
    "                    return False\n",
    "                break\n",
    "            j += 1\n",
    "                \n",
    "    return True\n",
    "    \n",
    "def recursive_build_mapping(\n",
    "    mapping: list[str],\n",
    "    nodes: list[Storage],\n",
    "    tensor2remaining: dict[Tensor, int],\n",
    "):\n",
    "    if not tensor2remaining or not any(tensor2remaining.values()):\n",
    "        yield mapping\n",
    "        return\n",
    "    \n",
    "    choices = [t for t in tensor2remaining if tensor2remaining[t] > 0]\n",
    "    choices += rank_variables\n",
    "    \n",
    "    for choice in choices:\n",
    "        mapping2 = mapping + [choice]\n",
    "        # Check for violation\n",
    "        if is_valid(mapping2):\n",
    "            if choice in tensors:\n",
    "                tensor2remaining[choice] -= 1\n",
    "            yield from recursive_build_mapping(mapping2, nodes, tensor2remaining)\n",
    "            if choice in tensors:\n",
    "                tensor2remaining[choice] += 1\n",
    "    \n",
    "    \n",
    "# If there are two back-to-back storages for the same tensor & the outer is\n",
    "# optional, then it is invalid.\n",
    "uneven_storages = [n for n in arch_nodes if n.constraints.storage.uneven]\n",
    "storage_choice_options = [s for s, _ in make_storage_choices_all_levels(arch_nodes, symbol_table)]\n",
    "\n",
    "groups = []\n",
    "current_group = []\n",
    "for node in arch_nodes:\n",
    "    if isinstance(node, Storage):\n",
    "        if current_group and not node.constraints.storage.uneven:\n",
    "            groups.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(node)\n",
    "if current_group:\n",
    "    groups.append(current_group)\n",
    "\n",
    "\n",
    "# main_memory = arch_nodes[0]\n",
    "# for storage_choices in storage_choice_options[8:]:\n",
    "#     print(f'{storage_choices}')\n",
    "\n",
    "#     mapping_base = list(storage_choices[main_memory.name])\n",
    "#     for group in groups:\n",
    "#         print(f'\\tGroup: {[n.name for n in group]}')\n",
    "#         group_names = [n.name for n in group]\n",
    "#         tensor2remaining = {t: 0 for t in tensors}\n",
    "#         for k, v in storage_choices.items():\n",
    "#             if k != main_memory.name:\n",
    "#                 for t in v:\n",
    "#                     tensor2remaining[t] += 1\n",
    "#         for mapping in recursive_build_mapping(mapping_base, group, tensor2remaining):\n",
    "#             print(f'\\t\\t{mapping}')\n",
    "    \n",
    "    \n",
    "def recursive_build_from_groups(groups, storage_choices, mapping_base=None):\n",
    "    mapping_base = [] if mapping_base is None else mapping_base\n",
    "    mapping_base = [x for x in mapping_base]\n",
    "    if not groups:\n",
    "        yield mapping_base\n",
    "        return\n",
    "    \n",
    "    group = groups[0]\n",
    "    tensor2remaining = {t: 0 for t in tensors}\n",
    "    for node in group:\n",
    "        for t in storage_choices[node.name]:\n",
    "            tensor2remaining[t] += 1\n",
    "\n",
    "    main_memory = group[0]\n",
    "    for t in storage_choices[main_memory.name]:\n",
    "        tensor2remaining[t] -= 1\n",
    "        mapping_base.append(t)\n",
    "            \n",
    "    for mapping in recursive_build_mapping(mapping_base, group, tensor2remaining):\n",
    "        yield from recursive_build_from_groups(groups[1:], storage_choices, mapping)\n",
    "    \n",
    "import time\n",
    "\n",
    "for storage_choices in storage_choice_options[:1]:\n",
    "    print(f'{storage_choices}')\n",
    "    main_memory = arch_nodes[0]\n",
    "\n",
    "    start_time = time.time()\n",
    "    mappings_count = 0\n",
    "    for mapping in recursive_build_from_groups(groups, storage_choices):\n",
    "        mappings_count += 1\n",
    "        print(mapping)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        mappings_per_second = mappings_count / elapsed_time if elapsed_time > 0 else 0\n",
    "        # print(f\"Generated {mappings_count} mappings in {elapsed_time:.2f} seconds ({mappings_per_second:.2f} mappings/second)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
