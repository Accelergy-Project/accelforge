'0.0':
  What is your name?:
    answer: Claude
    assumptions: []
    weight: 1
'0.1':
  What is your email address?:
    answer: claude@mit.edu
    assumptions: []
    weight: 1
'0.2':
  What is your kerberos?:
    answer: claude
    assumptions: []
    weight: 1
1.1.1:
  ? "\n    What is the computational intensity of the matrix\n    multiplication above given the rank shapes?\n    "
  : answer: 146.28571428571428
    assumptions: []
    weight: 1
1.1.2:
  What are the computational intensity 'knees' for the roofline model?:
    answer:
    - 62.5
    - 125.0
    - 187.5
    assumptions: []
    weight: 1
1.1.3:
  Based on each roofline model, what actual throughputs are achieved?:
    answer:
    - 4000.0
    - 8000.0
    - 9362.285714285714
    assumptions: []
    weight: 1
1.1.4:
  ? "\n    In order to achieve peak computational throughput of\n    12 kMAC/cycle, what DRAM bandwidth (in tensor-element/cycle) would be\n    required?\n    "
  : answer: 82.03125
    assumptions: []
    weight: 1
1.2.1:
  How many DRAM accesses are there in total?:
    answer: 402653184
    assumptions: []
    weight: 1
1.2.10:
  How many cycles would the computation units take to perform all MAC operations?:
    answer: 262144
    assumptions: []
    weight: 1
1.2.11:
  What minimum DRAM bandwidth is required to make the processing compute-bound?:
    answer: 530.0
    assumptions: []
    weight: 1
1.2.2:
  ? "\n    How many cycles would the computation units take to\n    perform all MAC operations?\n    "
  : answer: 134217728
    assumptions: []
    weight: 1
1.2.3:
  ? "\n    What minimum DRAM bandwidth is required to make the processing\n    compute-bound?\n    "
  : answer: 3
    assumptions: []
    weight: 1
1.2.4:
  How many cycles would the computation units take to perform all MAC operations?:
    answer: 262144
    assumptions: []
    weight: 1
1.2.5:
  What minimum DRAM bandwidth is required to make the processing compute-bound?:
    answer: 1536
    assumptions: []
    weight: 1
1.2.6:
  Do the parallel units access the same element of tensors A, B, and Z at any cycle?:
    answer:
    - true
    - false
    - false
    assumptions: []
    weight: 1
1.2.7:
  Assuming we can leverage spatial data reuse, how many DRAM accesses are there in total?:
    answer: 135004160
    assumptions: []
    weight: 1
1.2.8:
  What minimum DRAM bandwidth is required to make the processing compute-bound?:
    answer: 515.0
    assumptions: []
    weight: 1
1.2.9:
  Assuming we can leverage spatial data reuse, how many DRAM accesses are there in total?:
    answer: 138936320
    assumptions: []
    weight: 1
1.3.1:
  What is the maximum throughput in MAC/s given our workload shape?:
    answer: 256000000000.0
    assumptions: []
    weight: 1
1.4.1:
  How many MAC units are used in the mapping?:
    answer: 256
    assumptions: []
    weight: 1
1.4.2:
  Which tensor has spatial data reuse?:
    answer: Z
    assumptions: []
    weight: 1
1.4.3:
  ? "\n    For the tensor in your answer to 1.4.2, how many MAC operations are able to\n    be performed for each access (total reads and writes) of that tensor\n    element?\n    "
  : answer: 256
    assumptions: []
    weight: 1
2.1.1:
  What is the computational intensity of each Einsum in order as a list?:
    answer:
    - 204.8
    - 0.3333333333333333
    - 204.8
    assumptions: []
    weight: 1
2.1.2:
  What are the achieved throughputs for each Einsum in order as a list?:
    answer:
    - 12000.0
    - 21.333333333333332
    - 12000.0
    assumptions: []
    weight: 1
2.1.3:
  ? "\n    What is the overall computational intensity, with the fusion,\n    across all three Einsums?\n    "
  : answer: 256.25
    assumptions: []
    weight: 1
2.1.4:
  What is the overall achieved throughput with fusion?:
    answer: 12000.0
    assumptions: []
    weight: 1
2.2.1:
  ? "\n    Based on the storage nodes, are intermediate tensors X and Y kept in DRAM?\n    List each answer in order as a list.\n    "
  : answer:
    - true
    - true
    assumptions: []
    weight: 1
2.2.10:
  ? "\n    What is the lifetime of tensor B in the GLB?\n    List the set of Einsums during whose processing the tensor is kept\n    in GLB.\n    "
  : answer: !!set
      ? EinsumX null null null null null null null null null null null null null null null null null null null null null null
    assumptions: []
    weight: 1
2.2.2:
  ? "\n    Based on the storage nodes, are intermediate tensors X and Y kept in DRAM?\n    List each answer in order as a list.\n    "
  : answer:
    - false
    - true
    assumptions: []
    weight: 1
2.2.3:
  ? "\n    Where are the highest memory levels that keep tensors X and Y?\n    Please answer for each tensor in order as a list.\n    "
  : answer:
    - GLB
    - DRAM
    assumptions: []
    weight: 1
2.2.4:
  How much GLB capacity (in tensor-elements) is used to store tensor X tiles?:
    answer: 65536
    assumptions: []
    weight: 1
2.2.5:
  ? "\n    What is the lifetime of tensor X tile stored in the GLB?\n    List the set of Einsums during whose processing the tensor tile is kept\n    in GLB.\n    Hint: which Einsums use the tensor X, either as output or input?\n    "
  : answer: !!set
      ? EinsumY null null null null null null null null null null null null null null null null null null null null null null null null
      : null
      ? EinsumX null null null null null null null null null null null null null null null null null null null null null null null null
      : null
    assumptions: []
    weight: 1
2.2.6:
  ? "\n    Based on the storage nodes, are intermediate tensors X and Y kept in DRAM?\n    List each answer in order as a list.\n    "
  : answer:
    - false
    - false
    assumptions: []
    weight: 1
2.2.7:
  ? "\n    What are the highest memory levels that keep tensors X and Y?\n    Answer as a list in order.\n    "
  : answer:
    - GLB
    - GLB
    assumptions: []
    weight: 1
2.2.8:
  ? "\n    What is the lifetime of tensor X in the GLB?\n    List the set of Einsums during whose processing the tensor is kept\n    in GLB.\n    "
  : answer: !!set
      ? EinsumY null null null null null null null null null null null null null null null null null null null null null null null null
      ? EinsumX null null null null null null null null null null null null null null null null null null null null null null null null
    assumptions: []
    weight: 1
2.2.9:
  ? "\n    What is the lifetime of tensor Y in the GLB?\n    List the set of Einsums during whose processing the tensor is kept\n    in GLB.\n    "
  : answer: !!set
      ? EinsumY null null null null null null null null null null null null null null null null null null null null null null null
      ? EinsumZ null null null null null null null null null null null null null null null null null null null null null null null
    assumptions: []
    weight: 1
2.3.1:
  What rank of tensor X is partitioned to form tiles?:
    answer: nx
    assumptions: []
    weight: 1
2.3.10:
  ? "\n    What is the lifetime of tensor Y tile stored in the GLB? List the set of\n    Einsums during whose processing the tensor tile is kept in GLB.\n    "
  : answer: !!set
      ? EinsumY null null null null null null null null null null null null
      ? EinsumZ null null null null null null null null null null null null
    assumptions: []
    weight: 1
2.3.11:
  ? "\n    What is the lifetime of tensor B tile stored in the GLB?\n    List the set of Einsums during whose processing the tensor tile is kept\n    in GLB.\n    Hint: note the loop over m under the storage node \"GLB reuses B, D\".\n    What lifetime will allow us to reuse B across those iterations?\n    "
  : answer: !!set
      ? EinsumY null null null null null null null null null null null
      ? EinsumX null null null null null null null null null null null
      ? EinsumZ null null null null null null null null null null null
    assumptions: []
    weight: 1
2.3.2:
  How much GLB capacity (in tensor-elements) is used to keep tensor X tiles?:
    answer: 256
    assumptions: []
    weight: 1
2.3.3:
  ? "\n    How much smaller GLB capacity is used to store tensor X tiles compared to\n    the untiled version in Question 2.2.4? Report the ratio untiled/tiled.\n    "
  : answer: 256
    assumptions: []
    weight: 1
2.3.4:
  What rank of tensor X is partitioned to form tiles?:
    answer: m
    assumptions: []
    weight: 1
2.3.5:
  What rank of tensor Y is partitioned to form tiles?:
    answer: m
    assumptions: []
    weight: 1
2.3.6:
  ? "\n    List all ranks that, if partitioned, form tiles of *both* tensors X\n    and Y?\n    "
  : answer: !!set
      ? m null null null null null null null null null null null null null null null null
    assumptions: []
    weight: 1
2.3.7:
  How much GLB capacity is used to store tensor X tiles?:
    answer: 256
    assumptions: []
    weight: 1
2.3.8:
  How much GLB capacity is used to store tensor Y tiles?:
    answer: 256
    assumptions: []
    weight: 1
2.3.9:
  ? "\n    What is the lifetime of tensor X tile stored in the GLB? List the set of\n    Einsums during whose processing the tensor tile is kept in GLB.\n    "
  : answer: !!set
      ? EinsumY null null null null null null null null null null null null null
      ? EinsumX null null null null null null null null null null null null null
    assumptions: []
    weight: 1
2.4.1:
  ? "\n    True/False: Fusing more always increases GLB usage significantly (~2x)\n    compared to fusing some and unfused, regardless of workload shape.\n    "
  : answer: false
    assumptions: []
    weight: 1
2.4.2:
  ? "\n    True/False: Tiling shared tensors lead to significant reduction in GLB\n    usage for \"Fuse some\" in some workload shape.\n    "
  : answer: true
    assumptions: []
    weight: 1
2.4.3:
  ? "\n    True/False: Tiling shared tensors lead to significant reduction in GLB\n    usage for \"Fuse all\" in some workload shape.\n    "
  : answer: true
    assumptions: []
    weight: 1
2.4.4:
  ? "\n    In the mapping above, which one tensor constitute the most to GLB usage in\n    *both* workload shapes?\n\n    Hint: There is one tensor that contributes the most to GLB usage for the\n    M=512 workload.\n\n    More hint: Remember that placement of storage node relative to loop nodes\n    determine the shape of the tile kept in the memory level, and the tile is\n    what determines usage.\n    "
  : answer: !!set
      ? A null null null null null null null
    assumptions: []
    weight: 1
2.4.5:
  ? "\n    In the \"Fuse all, tiled\" mapping above, which tensor(s) are kept in their\n    entirety in the GLB?\n\n    Hint: look at the placement of storage nodes relative to loop nodes.\n    "
  : answer: !!set
      ? D null null null null null null
      ? B null null null null null null
    assumptions: []
    weight: 1
2.4.6:
  ? "\n    In the workload with M=512, which tensor(s) are the smallest tensors?\n\n    Do not consider the mapping in this question, base your answer only on the\n    workload.\n    "
  : answer: !!set
      ? D null null null null null
      ? B null null null null null
    assumptions: []
    weight: 1
2.4.7:
  ? "\n    True/False: The \"Fuse all, tiled\" mapping keeps only tiles of the largest\n    tensors in GLB, and keeps only smaller tensors in entirety.\n    "
  : answer: true
    assumptions: []
    weight: 1
3.1.1:
  Is the memory hierarchy area within the allowed area?:
    answer: true
    assumptions: []
    weight: 1
3.1.2:
  'Your score is:':
    answer: 100
    assumptions: []
    weight: 1
4.1.1:
  ? "\n    At what GlobalBuffer capacity is minimum DRAM accesses\n    reached?\n    Answer with the nearest power of ten (e.g., if the\n    GlobalBuffer capacity is 2.3e4, answer with 4.)\n    "
  : answer: 6
    assumptions: []
    weight: 1
4.1.2:
  ? "\n    True/False: does the default architecture configuration reach\n    the ski slope bound?\n    "
  : answer: false
    assumptions: []
    weight: 1
